{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da0d4313",
   "metadata": {},
   "source": [
    "# NEUROSYMBOLIC INTEGRATED WITH GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8729af5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… PyTorch loaded successfully\n",
      "âœ… tqdm loaded for progress bars\n",
      "âœ… PyTorch Geometric loaded successfully\n",
      "âœ… Using device: cuda\n",
      "ğŸš€ğŸ’ LAUNCHING OPTIMIZED 5-8M PARAMETER TRAINING ğŸ’ğŸš€\n",
      "ğŸš€ğŸ’ OPTIMIZED 5-8M PARAMETER TRAINING WITH PROGRESS BAR ğŸ’ğŸš€\n",
      "================================================================================\n",
      "ğŸ“‚ Loading data...\n",
      "ğŸ”„ Transferring to GPU...\n",
      "âœ… Setup complete: 19056 train, 4764 val\n",
      "   Train balance: 0.543\n",
      "   Val balance: 0.543\n",
      "ğŸ’ OPTIMIZED Model created:\n",
      "   Total parameters: 10,380,433 (10.38M)\n",
      "   Trainable parameters: 10,380,433 (10.38M)\n",
      "   âš ï¸ Outside 5-8M range\n",
      "================================================================================\n",
      "ğŸš€ STARTING OPTIMIZED TRAINING WITH DETAILED PROGRESS\n",
      "================================================================================\n",
      "Epoch  Time     Loss     Val Acc  Best Acc  LR         Status\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:   0%|          | 1/400 [00:12<1:23:22, 12.54s/epoch, Loss=1.2585, Val=50.13%, Best=50.13%, Patience=0/40, ETA=1.4h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1      12.5s    1.2585   50.13   % 50.13   % 2.55e-03   ğŸ† NEW BEST!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:   0%|          | 2/400 [00:29<1:39:44, 15.04s/epoch, Loss=1.6871, Val=53.86%, Best=53.86%, Patience=0/40, ETA=1.6h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2      16.8s    1.6871   53.86   % 53.86   % 2.70e-03   ğŸ† NEW BEST!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:   1%|          | 3/400 [00:46<1:45:05, 15.88s/epoch, Loss=1.4936, Val=51.74%, Best=53.86%, Patience=1/40, ETA=1.7h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3      16.9s    1.4936   51.74   % 53.86   % 2.95e-03   ğŸ“ˆ (1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:   1%|          | 4/400 [01:04<1:50:44, 16.78s/epoch, Loss=1.0233, Val=45.74%, Best=53.86%, Patience=2/40, ETA=1.8h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4      18.2s    1.0233   45.74   % 53.86   % 3.29e-03   ğŸ“ˆ (2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:   1%|â–         | 5/400 [01:22<1:52:59, 17.16s/epoch, Loss=0.9060, Val=45.74%, Best=53.86%, Patience=3/40, ETA=1.8h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5      17.8s    0.9060   45.74   % 53.86   % 3.71e-03   ğŸ“ˆ (3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:   2%|â–         | 6/400 [01:39<1:53:14, 17.24s/epoch, Loss=0.9544, Val=45.74%, Best=53.86%, Patience=4/40, ETA=1.8h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6      17.4s    0.9544   45.74   % 53.86   % 4.20e-03   ğŸ“ˆ (4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:   2%|â–         | 7/400 [01:56<1:52:34, 17.19s/epoch, Loss=0.9053, Val=52.60%, Best=53.86%, Patience=5/40, ETA=1.8h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7      17.1s    0.9053   52.60   % 53.86   % 4.74e-03   ğŸ“ˆ (5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:   2%|â–         | 8/400 [02:14<1:52:42, 17.25s/epoch, Loss=0.7730, Val=52.43%, Best=53.86%, Patience=6/40, ETA=1.8h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8      17.4s    0.7730   52.43   % 53.86   % 5.33e-03   â³ (6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:   2%|â–         | 9/400 [02:31<1:53:40, 17.44s/epoch, Loss=0.7972, Val=53.11%, Best=53.86%, Patience=7/40, ETA=1.8h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9      17.9s    0.7972   53.11   % 53.86   % 5.94e-03   â³ (7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:   2%|â–         | 10/400 [02:49<1:53:12, 17.42s/epoch, Loss=0.7576, Val=53.76%, Best=53.86%, Patience=8/40, ETA=1.8h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10     17.3s    0.7576   53.76   % 53.86   % 6.56e-03   â³ (8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:   3%|â–         | 11/400 [03:06<1:52:40, 17.38s/epoch, Loss=0.7496, Val=52.96%, Best=53.86%, Patience=9/40, ETA=1.9h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11     17.3s    0.7496   52.96   % 53.86   % 7.17e-03   â³ (9)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:   3%|â–         | 12/400 [03:25<1:54:30, 17.71s/epoch, Loss=0.7476, Val=54.51%, Best=54.51%, Patience=0/40, ETA=1.9h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12     17.6s    0.7476   54.51   % 54.51   % 7.76e-03   ğŸ† NEW BEST!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:   3%|â–         | 13/400 [03:38<1:45:49, 16.41s/epoch, Loss=0.7144, Val=54.60%, Best=54.60%, Patience=0/40, ETA=1.8h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13     13.4s    0.7144   54.60   % 54.60   % 8.30e-03   ğŸ† NEW BEST!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:   4%|â–         | 14/400 [03:54<1:44:03, 16.18s/epoch, Loss=0.7097, Val=54.79%, Best=54.79%, Patience=0/40, ETA=1.8h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14     15.6s    0.7097   54.79   % 54.79   % 8.79e-03   ğŸ† NEW BEST!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:   4%|â–         | 15/400 [04:10<1:43:49, 16.18s/epoch, Loss=0.6996, Val=55.67%, Best=55.67%, Patience=0/40, ETA=1.8h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15     16.2s    0.6996   55.67   % 55.67   % 9.21e-03   ğŸ† NEW BEST!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:   4%|â–         | 16/400 [04:25<1:42:29, 16.01s/epoch, Loss=0.6908, Val=54.53%, Best=55.67%, Patience=1/40, ETA=1.8h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16     15.6s    0.6908   54.53   % 55.67   % 9.55e-03   ğŸ“ˆ (1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:   4%|â–         | 17/400 [04:41<1:41:48, 15.95s/epoch, Loss=0.6774, Val=54.35%, Best=55.67%, Patience=2/40, ETA=1.7h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17     15.8s    0.6774   54.35   % 55.67   % 9.80e-03   ğŸ“ˆ (2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:   4%|â–         | 18/400 [04:57<1:41:01, 15.87s/epoch, Loss=0.6795, Val=54.39%, Best=55.67%, Patience=3/40, ETA=1.7h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18     15.7s    0.6795   54.39   % 55.67   % 9.95e-03   ğŸ“ˆ (3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:   5%|â–         | 19/400 [05:12<1:39:46, 15.71s/epoch, Loss=0.6724, Val=54.53%, Best=55.67%, Patience=4/40, ETA=1.7h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19     15.3s    0.6724   54.53   % 55.67   % 1.00e-02   ğŸ“ˆ (4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:   5%|â–Œ         | 20/400 [05:28<1:39:24, 15.70s/epoch, Loss=0.6671, Val=56.95%, Best=56.95%, Patience=0/40, ETA=1.7h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20     15.7s    0.6671   56.95   % 56.95   % 1.00e-02   ğŸ† NEW BEST!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:   5%|â–Œ         | 21/400 [05:44<1:39:12, 15.71s/epoch, Loss=0.6600, Val=57.43%, Best=57.43%, Patience=0/40, ETA=1.6h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21     15.7s    0.6600   57.43   % 57.43   % 1.00e-02   ğŸ† NEW BEST!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:   6%|â–Œ         | 22/400 [05:59<1:38:24, 15.62s/epoch, Loss=0.6577, Val=58.00%, Best=58.00%, Patience=0/40, ETA=1.6h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22     15.4s    0.6577   58.00   % 58.00   % 1.00e-02   ğŸ† NEW BEST!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:   6%|â–Œ         | 23/400 [06:15<1:37:52, 15.58s/epoch, Loss=0.6517, Val=57.12%, Best=58.00%, Patience=1/40, ETA=1.6h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23     15.5s    0.6517   57.12   % 58.00   % 1.00e-02   ğŸ“ˆ (1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:   6%|â–Œ         | 23/400 [06:30<1:37:52, 15.58s/epoch, Loss=0.6469, Val=57.87%, Best=58.00%, Patience=2/40, ETA=1.6h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24     15.5s    0.6469   57.87   % 58.00   % 1.00e-02   ğŸ“ˆ (2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:   6%|â–‹         | 25/400 [06:45<1:35:17, 15.25s/epoch, Loss=0.6407, Val=58.35%, Best=58.35%, Patience=0/40, ETA=1.6h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25     13.8s    0.6407   58.35   % 58.35   % 9.99e-03   ğŸ† NEW BEST!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:   6%|â–‹         | 26/400 [07:00<1:35:37, 15.34s/epoch, Loss=0.6367, Val=55.75%, Best=58.35%, Patience=1/40, ETA=1.6h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26     15.6s    0.6367   55.75   % 58.35   % 9.99e-03   ğŸ“ˆ (1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:   7%|â–‹         | 27/400 [07:16<1:35:46, 15.41s/epoch, Loss=0.6379, Val=59.47%, Best=59.47%, Patience=0/40, ETA=1.6h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27     15.6s    0.6379   59.47   % 59.47   % 9.99e-03   ğŸ† NEW BEST!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:   7%|â–‹         | 28/400 [07:32<1:35:56, 15.47s/epoch, Loss=0.6290, Val=58.02%, Best=59.47%, Patience=1/40, ETA=1.6h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28     15.6s    0.6290   58.02   % 59.47   % 9.99e-03   ğŸ“ˆ (1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:   7%|â–‹         | 29/400 [07:47<1:35:47, 15.49s/epoch, Loss=0.6273, Val=57.83%, Best=59.47%, Patience=2/40, ETA=1.6h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29     15.5s    0.6273   57.83   % 59.47   % 9.98e-03   ğŸ“ˆ (2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:   8%|â–Š         | 30/400 [08:03<1:36:05, 15.58s/epoch, Loss=0.6229, Val=59.89%, Best=59.89%, Patience=0/40, ETA=1.6h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30     15.8s    0.6229   59.89   % 59.89   % 9.98e-03   ğŸ† NEW BEST!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:   8%|â–Š         | 31/400 [08:19<1:37:31, 15.86s/epoch, Loss=0.6229, Val=59.21%, Best=59.89%, Patience=1/40, ETA=1.6h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31     16.5s    0.6229   59.21   % 59.89   % 9.98e-03   ğŸ“ˆ (1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:   8%|â–Š         | 32/400 [08:35<1:36:22, 15.71s/epoch, Loss=0.6110, Val=58.98%, Best=59.89%, Patience=2/40, ETA=1.6h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32     15.4s    0.6110   58.98   % 59.89   % 9.97e-03   ğŸ“ˆ (2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:   8%|â–Š         | 33/400 [08:50<1:36:01, 15.70s/epoch, Loss=0.6131, Val=62.53%, Best=62.53%, Patience=0/40, ETA=1.6h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33     15.7s    0.6131   62.53   % 62.53   % 9.97e-03   ğŸ† NEW BEST!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:   8%|â–Š         | 34/400 [09:06<1:36:01, 15.74s/epoch, Loss=0.6164, Val=64.88%, Best=64.88%, Patience=0/40, ETA=1.6h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34     15.8s    0.6164   64.88   % 64.88   % 9.96e-03   ğŸ† NEW BEST!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:   9%|â–‰         | 35/400 [09:22<1:35:09, 15.64s/epoch, Loss=0.6056, Val=63.16%, Best=64.88%, Patience=1/40, ETA=1.6h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35     15.4s    0.6056   63.16   % 64.88   % 9.96e-03   ğŸ“ˆ (1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:   9%|â–‰         | 35/400 [09:37<1:35:09, 15.64s/epoch, Loss=0.6042, Val=62.22%, Best=64.88%, Patience=2/40, ETA=1.6h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36     15.4s    0.6042   62.22   % 64.88   % 9.95e-03   ğŸ“ˆ (2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:   9%|â–‰         | 37/400 [09:49<1:27:07, 14.40s/epoch, Loss=0.5976, Val=63.01%, Best=64.88%, Patience=3/40, ETA=1.5h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37     10.9s    0.5976   63.01   % 64.88   % 9.94e-03   ğŸ“ˆ (3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:  10%|â–‰         | 38/400 [10:01<1:23:02, 13.77s/epoch, Loss=0.5875, Val=63.79%, Best=64.88%, Patience=4/40, ETA=1.5h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38     12.3s    0.5875   63.79   % 64.88   % 9.94e-03   ğŸ“ˆ (4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:  10%|â–‰         | 39/400 [10:14<1:20:03, 13.31s/epoch, Loss=0.5975, Val=66.37%, Best=66.37%, Patience=0/40, ETA=1.5h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39     12.2s    0.5975   66.37   % 66.37   % 9.93e-03   ğŸ† NEW BEST!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:  10%|â–ˆ         | 40/400 [10:26<1:17:56, 12.99s/epoch, Loss=0.5862, Val=66.98%, Best=66.98%, Patience=0/40, ETA=1.4h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40     12.2s    0.5862   66.98   % 66.98   % 9.93e-03   ğŸ† NEW BEST!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:  10%|â–ˆ         | 41/400 [10:38<1:16:23, 12.77s/epoch, Loss=0.5796, Val=66.94%, Best=66.98%, Patience=1/40, ETA=1.4h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41     12.2s    0.5796   66.94   % 66.98   % 9.92e-03   ğŸ“ˆ (1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:  10%|â–ˆ         | 42/400 [10:51<1:15:33, 12.66s/epoch, Loss=0.5768, Val=66.46%, Best=66.98%, Patience=2/40, ETA=1.3h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42     12.4s    0.5768   66.46   % 66.98   % 9.91e-03   ğŸ“ˆ (2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:  11%|â–ˆ         | 43/400 [11:03<1:15:09, 12.63s/epoch, Loss=0.5746, Val=67.23%, Best=67.23%, Patience=0/40, ETA=1.3h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43     12.6s    0.5746   67.23   % 67.23   % 9.90e-03   ğŸ† NEW BEST!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:  11%|â–ˆ         | 44/400 [11:16<1:14:47, 12.61s/epoch, Loss=0.5722, Val=69.14%, Best=69.14%, Patience=0/40, ETA=1.3h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44     12.5s    0.5722   69.14   % 69.14   % 9.89e-03   ğŸ† NEW BEST!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:  11%|â–ˆâ–        | 45/400 [11:28<1:14:01, 12.51s/epoch, Loss=0.5667, Val=68.98%, Best=69.14%, Patience=1/40, ETA=1.2h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45     12.3s    0.5667   68.98   % 69.14   % 9.89e-03   ğŸ“ˆ (1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:  12%|â–ˆâ–        | 46/400 [11:40<1:13:40, 12.49s/epoch, Loss=0.5620, Val=69.37%, Best=69.37%, Patience=0/40, ETA=1.2h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46     12.4s    0.5620   69.37   % 69.37   % 9.88e-03   ğŸ† NEW BEST!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:  12%|â–ˆâ–        | 47/400 [11:53<1:13:13, 12.45s/epoch, Loss=0.5607, Val=69.82%, Best=69.82%, Patience=0/40, ETA=1.2h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47     12.3s    0.5607   69.82   % 69.82   % 9.87e-03   ğŸ† NEW BEST!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:  12%|â–ˆâ–        | 48/400 [12:06<1:14:55, 12.77s/epoch, Loss=0.5591, Val=69.96%, Best=69.96%, Patience=0/40, ETA=1.2h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48     12.4s    0.5591   69.96   % 69.96   % 9.86e-03   ğŸ† NEW BEST!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:  12%|â–ˆâ–        | 49/400 [12:17<1:11:47, 12.27s/epoch, Loss=0.5520, Val=71.24%, Best=71.24%, Patience=0/40, ETA=1.2h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49     11.1s    0.5520   71.24   % 71.24   % 9.85e-03   ğŸ† NEW BEST!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:  12%|â–ˆâ–        | 50/400 [12:31<1:13:10, 12.54s/epoch, Loss=0.5538, Val=70.05%, Best=71.24%, Patience=1/40, ETA=1.2h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50     13.2s    0.5538   70.05   % 71.24   % 9.84e-03   ğŸ“ˆ (1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:  13%|â–ˆâ–        | 51/400 [12:44<1:14:22, 12.79s/epoch, Loss=0.5490, Val=70.47%, Best=71.24%, Patience=2/40, ETA=1.2h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51     13.3s    0.5490   70.47   % 71.24   % 9.83e-03   ğŸ“ˆ (2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:  13%|â–ˆâ–        | 52/400 [12:57<1:15:11, 12.96s/epoch, Loss=0.5462, Val=70.68%, Best=71.24%, Patience=3/40, ETA=1.2h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52     13.4s    0.5462   70.68   % 71.24   % 9.82e-03   ğŸ“ˆ (3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:  13%|â–ˆâ–        | 53/400 [13:11<1:15:30, 13.06s/epoch, Loss=0.5416, Val=70.97%, Best=71.24%, Patience=4/40, ETA=1.2h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53     13.3s    0.5416   70.97   % 71.24   % 9.80e-03   ğŸ“ˆ (4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:  14%|â–ˆâ–        | 54/400 [13:24<1:16:00, 13.18s/epoch, Loss=0.5385, Val=72.96%, Best=72.96%, Patience=0/40, ETA=1.2h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54     13.5s    0.5385   72.96   % 72.96   % 9.79e-03   ğŸ† NEW BEST!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:  14%|â–ˆâ–        | 55/400 [13:37<1:16:14, 13.26s/epoch, Loss=0.5368, Val=72.59%, Best=72.96%, Patience=1/40, ETA=1.2h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55     13.4s    0.5368   72.59   % 72.96   % 9.78e-03   ğŸ“ˆ (1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:  14%|â–ˆâ–        | 56/400 [13:57<1:26:37, 15.11s/epoch, Loss=0.5388, Val=73.28%, Best=73.28%, Patience=0/40, ETA=1.3h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56     19.4s    0.5388   73.28   % 73.28   % 9.77e-03   ğŸ† NEW BEST!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:  14%|â–ˆâ–        | 57/400 [14:16<1:33:21, 16.33s/epoch, Loss=0.5329, Val=74.08%, Best=74.08%, Patience=0/40, ETA=1.4h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57     19.2s    0.5329   74.08   % 74.08   % 9.76e-03   ğŸ† NEW BEST!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:  14%|â–ˆâ–        | 58/400 [14:35<1:37:36, 17.12s/epoch, Loss=0.5304, Val=73.97%, Best=74.08%, Patience=1/40, ETA=1.4h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58     19.0s    0.5304   73.97   % 74.08   % 9.74e-03   ğŸ“ˆ (1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:  15%|â–ˆâ–        | 59/400 [14:54<1:40:30, 17.69s/epoch, Loss=0.5273, Val=73.95%, Best=74.08%, Patience=2/40, ETA=1.5h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59     19.0s    0.5273   73.95   % 74.08   % 9.73e-03   ğŸ“ˆ (2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:  15%|â–ˆâ–        | 59/400 [15:13<1:40:30, 17.69s/epoch, Loss=0.5314, Val=75.23%, Best=75.23%, Patience=0/40, ETA=1.5h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60     19.1s    0.5314   75.23   % 75.23   % 9.72e-03   ğŸ† NEW BEST!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:  15%|â–ˆâ–Œ        | 61/400 [15:33<1:43:59, 18.41s/epoch, Loss=0.5273, Val=75.99%, Best=75.99%, Patience=0/40, ETA=1.6h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61     17.9s    0.5273   75.99   % 75.99   % 9.70e-03   ğŸ† NEW BEST!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:  16%|â–ˆâ–Œ        | 62/400 [15:51<1:43:40, 18.40s/epoch, Loss=0.5206, Val=74.75%, Best=75.99%, Patience=1/40, ETA=1.6h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62     18.4s    0.5206   74.75   % 75.99   % 9.69e-03   ğŸ“ˆ (1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:  16%|â–ˆâ–Œ        | 63/400 [16:10<1:43:47, 18.48s/epoch, Loss=0.5168, Val=74.54%, Best=75.99%, Patience=2/40, ETA=1.7h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63     18.7s    0.5168   74.54   % 75.99   % 9.67e-03   ğŸ“ˆ (2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:  16%|â–ˆâ–Œ        | 64/400 [16:28<1:43:39, 18.51s/epoch, Loss=0.5188, Val=75.36%, Best=75.99%, Patience=3/40, ETA=1.7h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64     18.6s    0.5188   75.36   % 75.99   % 9.66e-03   ğŸ“ˆ (3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:  16%|â–ˆâ–‹        | 65/400 [16:47<1:43:27, 18.53s/epoch, Loss=0.5201, Val=75.19%, Best=75.99%, Patience=4/40, ETA=1.7h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65     18.6s    0.5201   75.19   % 75.99   % 9.64e-03   ğŸ“ˆ (4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:  16%|â–ˆâ–‹        | 66/400 [17:06<1:43:18, 18.56s/epoch, Loss=0.5192, Val=76.05%, Best=75.99%, Patience=5/40, ETA=1.7h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66     18.6s    0.5192   76.05   % 75.99   % 9.63e-03   ğŸ“ˆ (5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:  17%|â–ˆâ–‹        | 67/400 [17:24<1:43:30, 18.65s/epoch, Loss=0.5180, Val=76.32%, Best=76.32%, Patience=0/40, ETA=1.7h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67     18.9s    0.5180   76.32   % 76.32   % 9.61e-03   ğŸ† NEW BEST!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:  17%|â–ˆâ–‹        | 68/400 [17:43<1:43:16, 18.66s/epoch, Loss=0.5095, Val=75.76%, Best=76.32%, Patience=1/40, ETA=1.7h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68     18.7s    0.5095   75.76   % 76.32   % 9.60e-03   ğŸ“ˆ (1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:  17%|â–ˆâ–‹        | 69/400 [18:02<1:42:50, 18.64s/epoch, Loss=0.5088, Val=76.18%, Best=76.32%, Patience=2/40, ETA=1.7h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69     18.6s    0.5088   76.18   % 76.32   % 9.58e-03   ğŸ“ˆ (2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:  18%|â–ˆâ–Š        | 70/400 [18:20<1:42:21, 18.61s/epoch, Loss=0.5100, Val=76.26%, Best=76.32%, Patience=3/40, ETA=1.7h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70     18.5s    0.5100   76.26   % 76.32   % 9.56e-03   ğŸ“ˆ (3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:  18%|â–ˆâ–Š        | 71/400 [18:39<1:42:01, 18.61s/epoch, Loss=0.5047, Val=76.26%, Best=76.32%, Patience=4/40, ETA=1.7h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71     18.6s    0.5047   76.26   % 76.32   % 9.55e-03   ğŸ“ˆ (4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:  18%|â–ˆâ–Š        | 71/400 [18:57<1:42:01, 18.61s/epoch, Loss=0.5026, Val=76.36%, Best=76.32%, Patience=5/40, ETA=1.7h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72     18.6s    0.5026   76.36   % 76.32   % 9.53e-03   ğŸ“ˆ (5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:  18%|â–ˆâ–Š        | 73/400 [19:17<1:42:53, 18.88s/epoch, Loss=0.5005, Val=76.70%, Best=76.70%, Patience=0/40, ETA=1.7h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73     18.5s    0.5005   76.70   % 76.70   % 9.51e-03   ğŸ† NEW BEST!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:  18%|â–ˆâ–Š        | 74/400 [19:36<1:42:46, 18.91s/epoch, Loss=0.5015, Val=77.10%, Best=77.10%, Patience=0/40, ETA=1.7h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74     19.0s    0.5015   77.10   % 77.10   % 9.49e-03   ğŸ† NEW BEST!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:  19%|â–ˆâ–‰        | 75/400 [19:56<1:42:58, 19.01s/epoch, Loss=0.4960, Val=76.78%, Best=77.10%, Patience=1/40, ETA=1.7h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75     19.2s    0.4960   76.78   % 77.10   % 9.48e-03   ğŸ“ˆ (1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:  19%|â–ˆâ–‰        | 76/400 [20:15<1:42:55, 19.06s/epoch, Loss=0.5004, Val=76.36%, Best=77.10%, Patience=2/40, ETA=1.7h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76     19.2s    0.5004   76.36   % 77.10   % 9.46e-03   ğŸ“ˆ (2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:  19%|â–ˆâ–‰        | 77/400 [20:34<1:42:49, 19.10s/epoch, Loss=0.4981, Val=77.02%, Best=77.10%, Patience=3/40, ETA=1.7h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77     19.2s    0.4981   77.02   % 77.10   % 9.44e-03   ğŸ“ˆ (3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:  20%|â–ˆâ–‰        | 78/400 [20:53<1:43:00, 19.20s/epoch, Loss=0.4995, Val=77.43%, Best=77.43%, Patience=0/40, ETA=1.7h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78     19.4s    0.4995   77.43   % 77.43   % 9.42e-03   ğŸ† NEW BEST!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:  20%|â–ˆâ–‰        | 79/400 [21:13<1:42:49, 19.22s/epoch, Loss=0.4987, Val=77.14%, Best=77.43%, Patience=1/40, ETA=1.7h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79     19.3s    0.4987   77.14   % 77.43   % 9.40e-03   ğŸ“ˆ (1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:  20%|â–ˆâ–ˆ        | 80/400 [21:32<1:42:37, 19.24s/epoch, Loss=0.4943, Val=77.16%, Best=77.43%, Patience=2/40, ETA=1.7h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80     19.3s    0.4943   77.16   % 77.43   % 9.38e-03   ğŸ“ˆ (2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:  20%|â–ˆâ–ˆ        | 81/400 [21:51<1:42:02, 19.19s/epoch, Loss=0.4918, Val=77.18%, Best=77.43%, Patience=3/40, ETA=1.7h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81     19.1s    0.4918   77.18   % 77.43   % 9.36e-03   ğŸ“ˆ (3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:  20%|â–ˆâ–ˆ        | 82/400 [22:10<1:41:40, 19.18s/epoch, Loss=0.4911, Val=76.87%, Best=77.43%, Patience=4/40, ETA=1.7h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82     19.2s    0.4911   76.87   % 77.43   % 9.34e-03   ğŸ“ˆ (4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:  21%|â–ˆâ–ˆ        | 83/400 [22:30<1:41:28, 19.21s/epoch, Loss=0.4959, Val=77.31%, Best=77.43%, Patience=5/40, ETA=1.7h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83     19.3s    0.4959   77.31   % 77.43   % 9.32e-03   ğŸ“ˆ (5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:  21%|â–ˆâ–ˆ        | 83/400 [22:49<1:41:28, 19.21s/epoch, Loss=0.4930, Val=77.39%, Best=77.43%, Patience=6/40, ETA=1.7h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84     19.2s    0.4930   77.39   % 77.43   % 9.30e-03   â³ (6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:  21%|â–ˆâ–ˆâ–       | 85/400 [23:08<1:39:54, 19.03s/epoch, Loss=0.4875, Val=77.35%, Best=77.43%, Patience=7/40, ETA=1.7h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85     17.5s    0.4875   77.35   % 77.43   % 9.28e-03   â³ (7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:  22%|â–ˆâ–ˆâ–       | 86/400 [23:25<1:36:32, 18.45s/epoch, Loss=0.4906, Val=77.10%, Best=77.43%, Patience=8/40, ETA=1.6h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86     17.1s    0.4906   77.10   % 77.43   % 9.25e-03   â³ (8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:  22%|â–ˆâ–ˆâ–       | 87/400 [23:42<1:34:23, 18.09s/epoch, Loss=0.4860, Val=77.18%, Best=77.43%, Patience=9/40, ETA=1.6h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87     17.3s    0.4860   77.18   % 77.43   % 9.23e-03   â³ (9)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:  22%|â–ˆâ–ˆâ–       | 88/400 [23:59<1:32:43, 17.83s/epoch, Loss=0.4863, Val=77.50%, Best=77.43%, Patience=10/40, ETA=1.6h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88     17.2s    0.4863   77.50   % 77.43   % 9.21e-03   â³ (10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:  22%|â–ˆâ–ˆâ–       | 89/400 [24:17<1:32:03, 17.76s/epoch, Loss=0.4856, Val=77.71%, Best=77.71%, Patience=0/40, ETA=1.6h] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89     17.6s    0.4856   77.71   % 77.71   % 9.19e-03   ğŸ† NEW BEST!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:  22%|â–ˆâ–ˆâ–       | 90/400 [24:34<1:31:13, 17.66s/epoch, Loss=0.4830, Val=77.77%, Best=77.71%, Patience=1/40, ETA=1.6h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90     17.4s    0.4830   77.77   % 77.71   % 9.17e-03   ğŸ“ˆ (1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:  23%|â–ˆâ–ˆâ–       | 91/400 [24:52<1:30:20, 17.54s/epoch, Loss=0.4834, Val=77.58%, Best=77.71%, Patience=2/40, ETA=1.5h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91     17.3s    0.4834   77.58   % 77.71   % 9.14e-03   ğŸ“ˆ (2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:  23%|â–ˆâ–ˆâ–       | 92/400 [25:09<1:29:34, 17.45s/epoch, Loss=0.4853, Val=77.73%, Best=77.71%, Patience=3/40, ETA=1.5h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92     17.2s    0.4853   77.73   % 77.71   % 9.12e-03   ğŸ“ˆ (3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:  23%|â–ˆâ–ˆâ–       | 93/400 [25:26<1:29:14, 17.44s/epoch, Loss=0.4787, Val=77.81%, Best=77.81%, Patience=0/40, ETA=1.5h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93     17.4s    0.4787   77.81   % 77.81   % 9.10e-03   ğŸ† NEW BEST!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:  24%|â–ˆâ–ˆâ–       | 94/400 [25:44<1:29:05, 17.47s/epoch, Loss=0.4835, Val=77.98%, Best=77.98%, Patience=0/40, ETA=1.5h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94     17.5s    0.4835   77.98   % 77.98   % 9.07e-03   ğŸ† NEW BEST!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:  24%|â–ˆâ–ˆâ–       | 95/400 [26:01<1:28:48, 17.47s/epoch, Loss=0.4833, Val=78.13%, Best=78.13%, Patience=0/40, ETA=1.5h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95     17.5s    0.4833   78.13   % 78.13   % 9.05e-03   ğŸ† NEW BEST!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:  24%|â–ˆâ–ˆâ–       | 95/400 [26:19<1:28:48, 17.47s/epoch, Loss=0.4801, Val=78.25%, Best=78.25%, Patience=0/40, ETA=1.5h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96     17.3s    0.4801   78.25   % 78.25   % 9.02e-03   ğŸ† NEW BEST!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:  24%|â–ˆâ–ˆâ–       | 97/400 [26:37<1:28:42, 17.57s/epoch, Loss=0.4783, Val=78.11%, Best=78.25%, Patience=1/40, ETA=1.5h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97     17.3s    0.4783   78.11   % 78.25   % 9.00e-03   ğŸ“ˆ (1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:  24%|â–ˆâ–ˆâ–       | 98/400 [26:56<1:30:47, 18.04s/epoch, Loss=0.4799, Val=77.88%, Best=78.25%, Patience=2/40, ETA=1.5h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98     19.1s    0.4799   77.88   % 78.25   % 8.97e-03   ğŸ“ˆ (2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:  25%|â–ˆâ–ˆâ–       | 99/400 [27:15<1:32:23, 18.42s/epoch, Loss=0.4785, Val=78.09%, Best=78.25%, Patience=3/40, ETA=1.5h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99     19.3s    0.4785   78.09   % 78.25   % 8.95e-03   ğŸ“ˆ (3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:  25%|â–ˆâ–ˆâ–Œ       | 100/400 [27:35<1:33:31, 18.70s/epoch, Loss=0.4787, Val=78.27%, Best=78.25%, Patience=4/40, ETA=1.5h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100    19.4s    0.4787   78.27   % 78.25   % 8.92e-03   ğŸ“ˆ (4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:  25%|â–ˆâ–ˆâ–Œ       | 101/400 [27:54<1:34:13, 18.91s/epoch, Loss=0.4762, Val=78.25%, Best=78.25%, Patience=5/40, ETA=1.5h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101    19.4s    0.4762   78.25   % 78.25   % 8.90e-03   ğŸ“ˆ (5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:  26%|â–ˆâ–ˆâ–Œ       | 102/400 [28:13<1:34:36, 19.05s/epoch, Loss=0.4763, Val=78.13%, Best=78.25%, Patience=6/40, ETA=1.5h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102    19.4s    0.4763   78.13   % 78.25   % 8.87e-03   â³ (6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:  26%|â–ˆâ–ˆâ–Œ       | 103/400 [28:33<1:34:47, 19.15s/epoch, Loss=0.4771, Val=77.85%, Best=78.25%, Patience=7/40, ETA=1.5h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103    19.4s    0.4771   77.85   % 78.25   % 8.85e-03   â³ (7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:  26%|â–ˆâ–ˆâ–Œ       | 104/400 [28:52<1:34:56, 19.25s/epoch, Loss=0.4757, Val=77.81%, Best=78.25%, Patience=8/40, ETA=1.5h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104    19.5s    0.4757   77.81   % 78.25   % 8.82e-03   â³ (8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:  26%|â–ˆâ–ˆâ–‹       | 105/400 [29:12<1:34:50, 19.29s/epoch, Loss=0.4747, Val=78.00%, Best=78.25%, Patience=9/40, ETA=1.6h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105    19.4s    0.4747   78.00   % 78.25   % 8.79e-03   â³ (9)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:  26%|â–ˆâ–ˆâ–‹       | 106/400 [29:31<1:34:33, 19.30s/epoch, Loss=0.4745, Val=77.83%, Best=78.25%, Patience=10/40, ETA=1.6h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106    19.3s    0.4745   77.83   % 78.25   % 8.77e-03   â³ (10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:  27%|â–ˆâ–ˆâ–‹       | 107/400 [29:50<1:34:22, 19.33s/epoch, Loss=0.4714, Val=77.90%, Best=78.25%, Patience=11/40, ETA=1.6h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107    19.4s    0.4714   77.90   % 78.25   % 8.74e-03   â³ (11)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:  27%|â–ˆâ–ˆâ–‹       | 107/400 [30:10<1:34:22, 19.33s/epoch, Loss=0.4748, Val=77.75%, Best=78.25%, Patience=12/40, ETA=1.6h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108    19.4s    0.4748   77.75   % 78.25   % 8.71e-03   â³ (12)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:  27%|â–ˆâ–ˆâ–‹       | 109/400 [30:29<1:32:40, 19.11s/epoch, Loss=0.4746, Val=77.79%, Best=78.25%, Patience=13/40, ETA=1.6h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109    17.4s    0.4746   77.79   % 78.25   % 8.68e-03   â³ (13)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:  28%|â–ˆâ–ˆâ–Š       | 110/400 [30:47<1:31:08, 18.86s/epoch, Loss=0.4698, Val=77.90%, Best=78.25%, Patience=14/40, ETA=1.5h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110    18.3s    0.4698   77.90   % 78.25   % 8.65e-03   â³ (14)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:  28%|â–ˆâ–ˆâ–Š       | 111/400 [31:05<1:29:23, 18.56s/epoch, Loss=0.4718, Val=78.02%, Best=78.25%, Patience=15/40, ETA=1.5h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111    17.9s    0.4718   78.02   % 78.25   % 8.63e-03   â³ (15)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:  28%|â–ˆâ–ˆâ–Š       | 112/400 [31:23<1:28:09, 18.37s/epoch, Loss=0.4708, Val=77.85%, Best=78.25%, Patience=16/40, ETA=1.5h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112    17.9s    0.4708   77.85   % 78.25   % 8.60e-03   âš ï¸ (16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:  28%|â–ˆâ–ˆâ–Š       | 113/400 [31:41<1:27:08, 18.22s/epoch, Loss=0.4699, Val=78.11%, Best=78.25%, Patience=17/40, ETA=1.5h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113    17.9s    0.4699   78.11   % 78.25   % 8.57e-03   âš ï¸ (17)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:  28%|â–ˆâ–ˆâ–Š       | 114/400 [31:59<1:26:46, 18.21s/epoch, Loss=0.4720, Val=78.34%, Best=78.34%, Patience=0/40, ETA=1.5h] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114    18.2s    0.4720   78.34   % 78.34   % 8.54e-03   ğŸ† NEW BEST!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:  29%|â–ˆâ–ˆâ–‰       | 115/400 [32:17<1:26:29, 18.21s/epoch, Loss=0.4714, Val=78.57%, Best=78.57%, Patience=0/40, ETA=1.5h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115    18.2s    0.4714   78.57   % 78.57   % 8.51e-03   ğŸ† NEW BEST!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:  29%|â–ˆâ–ˆâ–‰       | 116/400 [32:35<1:25:41, 18.10s/epoch, Loss=0.4667, Val=78.57%, Best=78.57%, Patience=1/40, ETA=1.4h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116    17.9s    0.4667   78.57   % 78.57   % 8.48e-03   ğŸ“ˆ (1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:  29%|â–ˆâ–ˆâ–‰       | 117/400 [32:53<1:25:09, 18.06s/epoch, Loss=0.4697, Val=78.51%, Best=78.57%, Patience=2/40, ETA=1.4h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "117    17.9s    0.4697   78.51   % 78.57   % 8.45e-03   ğŸ“ˆ (2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:  30%|â–ˆâ–ˆâ–‰       | 118/400 [33:11<1:24:53, 18.06s/epoch, Loss=0.4680, Val=78.36%, Best=78.57%, Patience=3/40, ETA=1.4h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118    18.1s    0.4680   78.36   % 78.57   % 8.42e-03   ğŸ“ˆ (3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:  30%|â–ˆâ–ˆâ–‰       | 119/400 [33:29<1:24:23, 18.02s/epoch, Loss=0.4681, Val=78.38%, Best=78.57%, Patience=4/40, ETA=1.4h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119    17.9s    0.4681   78.38   % 78.57   % 8.39e-03   ğŸ“ˆ (4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:  30%|â–ˆâ–ˆâ–‰       | 119/400 [33:47<1:24:23, 18.02s/epoch, Loss=0.4684, Val=78.23%, Best=78.57%, Patience=5/40, ETA=1.4h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120    17.9s    0.4684   78.23   % 78.57   % 8.36e-03   ğŸ“ˆ (5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:  30%|â–ˆâ–ˆâ–ˆ       | 121/400 [34:06<1:24:22, 18.14s/epoch, Loss=0.4675, Val=78.42%, Best=78.57%, Patience=6/40, ETA=1.4h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "121    17.7s    0.4675   78.42   % 78.57   % 8.33e-03   â³ (6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:  30%|â–ˆâ–ˆâ–ˆ       | 122/400 [34:25<1:26:00, 18.56s/epoch, Loss=0.4710, Val=78.76%, Best=78.76%, Patience=0/40, ETA=1.4h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122    19.5s    0.4710   78.76   % 78.76   % 8.30e-03   ğŸ† NEW BEST!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:  31%|â–ˆâ–ˆâ–ˆ       | 123/400 [34:45<1:27:23, 18.93s/epoch, Loss=0.4713, Val=78.32%, Best=78.76%, Patience=1/40, ETA=1.4h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123    19.8s    0.4713   78.32   % 78.76   % 8.27e-03   ğŸ“ˆ (1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:  31%|â–ˆâ–ˆâ–ˆ       | 124/400 [35:05<1:28:29, 19.24s/epoch, Loss=0.4675, Val=78.04%, Best=78.76%, Patience=2/40, ETA=1.4h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124    19.9s    0.4675   78.04   % 78.76   % 8.24e-03   ğŸ“ˆ (2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:  31%|â–ˆâ–ˆâ–ˆâ–      | 125/400 [35:25<1:28:55, 19.40s/epoch, Loss=0.4689, Val=77.96%, Best=78.76%, Patience=3/40, ETA=1.4h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125    19.8s    0.4689   77.96   % 78.76   % 8.21e-03   ğŸ“ˆ (3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:  32%|â–ˆâ–ˆâ–ˆâ–      | 126/400 [35:45<1:29:34, 19.61s/epoch, Loss=0.4674, Val=78.27%, Best=78.76%, Patience=4/40, ETA=1.4h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126    20.1s    0.4674   78.27   % 78.76   % 8.17e-03   ğŸ“ˆ (4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:  32%|â–ˆâ–ˆâ–ˆâ–      | 127/400 [36:05<1:29:31, 19.67s/epoch, Loss=0.4661, Val=78.21%, Best=78.76%, Patience=5/40, ETA=1.4h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127    19.8s    0.4661   78.21   % 78.76   % 8.14e-03   ğŸ“ˆ (5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:  32%|â–ˆâ–ˆâ–ˆâ–      | 128/400 [36:25<1:29:37, 19.77s/epoch, Loss=0.4674, Val=78.34%, Best=78.76%, Patience=6/40, ETA=1.5h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128    20.0s    0.4674   78.34   % 78.76   % 8.11e-03   â³ (6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:  32%|â–ˆâ–ˆâ–ˆâ–      | 129/400 [36:44<1:29:18, 19.77s/epoch, Loss=0.4655, Val=78.46%, Best=78.76%, Patience=7/40, ETA=1.5h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129    19.8s    0.4655   78.46   % 78.76   % 8.08e-03   â³ (7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:  32%|â–ˆâ–ˆâ–ˆâ–      | 130/400 [37:04<1:29:02, 19.79s/epoch, Loss=0.4660, Val=78.44%, Best=78.76%, Patience=8/40, ETA=1.5h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130    19.8s    0.4660   78.44   % 78.76   % 8.04e-03   â³ (8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:  33%|â–ˆâ–ˆâ–ˆâ–      | 131/400 [37:24<1:28:48, 19.81s/epoch, Loss=0.4639, Val=78.38%, Best=78.76%, Patience=9/40, ETA=1.5h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131    19.9s    0.4639   78.38   % 78.76   % 8.01e-03   â³ (9)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:  33%|â–ˆâ–ˆâ–ˆâ–      | 131/400 [37:44<1:28:48, 19.81s/epoch, Loss=0.4656, Val=78.25%, Best=78.76%, Patience=10/40, ETA=1.5h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132    19.8s    0.4656   78.25   % 78.76   % 7.98e-03   â³ (10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:  33%|â–ˆâ–ˆâ–ˆâ–      | 133/400 [38:02<1:25:52, 19.30s/epoch, Loss=0.4649, Val=78.30%, Best=78.76%, Patience=11/40, ETA=1.5h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133    17.4s    0.4649   78.30   % 78.76   % 7.95e-03   â³ (11)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:  34%|â–ˆâ–ˆâ–ˆâ–      | 134/400 [38:22<1:25:53, 19.37s/epoch, Loss=0.4635, Val=78.42%, Best=78.76%, Patience=12/40, ETA=1.4h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "134    19.5s    0.4635   78.42   % 78.76   % 7.91e-03   â³ (12)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:  34%|â–ˆâ–ˆâ–ˆâ–      | 135/400 [38:41<1:25:34, 19.38s/epoch, Loss=0.4647, Val=78.36%, Best=78.76%, Patience=13/40, ETA=1.4h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135    19.4s    0.4647   78.36   % 78.76   % 7.88e-03   â³ (13)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:  34%|â–ˆâ–ˆâ–ˆâ–      | 136/400 [39:01<1:25:21, 19.40s/epoch, Loss=0.4651, Val=78.46%, Best=78.76%, Patience=14/40, ETA=1.4h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136    19.4s    0.4651   78.46   % 78.76   % 7.84e-03   â³ (14)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:  34%|â–ˆâ–ˆâ–ˆâ–      | 137/400 [39:20<1:25:06, 19.42s/epoch, Loss=0.4635, Val=78.65%, Best=78.76%, Patience=15/40, ETA=1.4h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137    19.5s    0.4635   78.65   % 78.76   % 7.81e-03   â³ (15)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:  34%|â–ˆâ–ˆâ–ˆâ–      | 138/400 [39:40<1:24:40, 19.39s/epoch, Loss=0.4641, Val=78.78%, Best=78.76%, Patience=16/40, ETA=1.4h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138    19.3s    0.4641   78.78   % 78.76   % 7.78e-03   âš ï¸ (16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:  35%|â–ˆâ–ˆâ–ˆâ–      | 139/400 [39:59<1:24:23, 19.40s/epoch, Loss=0.4627, Val=78.67%, Best=78.76%, Patience=17/40, ETA=1.4h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "139    19.4s    0.4627   78.67   % 78.76   % 7.74e-03   âš ï¸ (17)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 140/400 [40:18<1:24:00, 19.39s/epoch, Loss=0.4634, Val=78.55%, Best=78.76%, Patience=18/40, ETA=1.4h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140    19.3s    0.4634   78.55   % 78.76   % 7.71e-03   âš ï¸ (18)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 141/400 [40:38<1:23:43, 19.40s/epoch, Loss=0.4640, Val=78.48%, Best=78.76%, Patience=19/40, ETA=1.4h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141    19.4s    0.4640   78.48   % 78.76   % 7.67e-03   âš ï¸ (19)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 142/400 [40:57<1:23:21, 19.38s/epoch, Loss=0.4634, Val=78.17%, Best=78.76%, Patience=20/40, ETA=1.4h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "142    19.4s    0.4634   78.17   % 78.76   % 7.64e-03   âš ï¸ (20)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 143/400 [41:17<1:23:12, 19.42s/epoch, Loss=0.4611, Val=78.21%, Best=78.76%, Patience=21/40, ETA=1.4h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143    19.5s    0.4611   78.21   % 78.76   % 7.60e-03   âš ï¸ (21)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 143/400 [41:36<1:23:12, 19.42s/epoch, Loss=0.4634, Val=78.15%, Best=78.76%, Patience=22/40, ETA=1.4h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144    19.3s    0.4634   78.15   % 78.76   % 7.57e-03   âš ï¸ (22)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 145/400 [41:55<1:21:50, 19.26s/epoch, Loss=0.4598, Val=78.13%, Best=78.76%, Patience=23/40, ETA=1.4h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "145    18.2s    0.4598   78.13   % 78.76   % 7.53e-03   âš ï¸ (23)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 146/400 [42:16<1:23:42, 19.77s/epoch, Loss=0.4634, Val=78.17%, Best=78.76%, Patience=24/40, ETA=1.4h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "146    21.0s    0.4634   78.17   % 78.76   % 7.50e-03   âš ï¸ (24)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 147/400 [42:37<1:25:09, 20.19s/epoch, Loss=0.4617, Val=78.19%, Best=78.76%, Patience=25/40, ETA=1.4h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "147    21.2s    0.4617   78.19   % 78.76   % 7.46e-03   âš ï¸ (25)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 148/400 [42:58<1:25:55, 20.46s/epoch, Loss=0.4607, Val=78.09%, Best=78.76%, Patience=26/40, ETA=1.4h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "148    21.1s    0.4607   78.09   % 78.76   % 7.42e-03   âš ï¸ (26)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 149/400 [43:20<1:26:22, 20.65s/epoch, Loss=0.4589, Val=78.11%, Best=78.76%, Patience=27/40, ETA=1.4h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "149    21.1s    0.4589   78.11   % 78.76   % 7.39e-03   âš ï¸ (27)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 150/400 [43:41<1:26:52, 20.85s/epoch, Loss=0.4593, Val=78.17%, Best=78.76%, Patience=28/40, ETA=1.4h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150    21.3s    0.4593   78.17   % 78.76   % 7.35e-03   âš ï¸ (28)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 151/400 [44:02<1:26:54, 20.94s/epoch, Loss=0.4598, Val=78.44%, Best=78.76%, Patience=29/40, ETA=1.4h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151    21.2s    0.4598   78.44   % 78.76   % 7.32e-03   âš ï¸ (29)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 152/400 [44:23<1:26:56, 21.04s/epoch, Loss=0.4611, Val=78.19%, Best=78.76%, Patience=30/40, ETA=1.4h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152    21.2s    0.4611   78.19   % 78.76   % 7.28e-03   âš ï¸ (30)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 153/400 [44:44<1:26:50, 21.10s/epoch, Loss=0.4610, Val=78.17%, Best=78.76%, Patience=31/40, ETA=1.4h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153    21.2s    0.4610   78.17   % 78.76   % 7.24e-03   âš ï¸ (31)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 154/400 [45:06<1:26:39, 21.14s/epoch, Loss=0.4623, Val=78.42%, Best=78.76%, Patience=32/40, ETA=1.4h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "154    21.2s    0.4623   78.42   % 78.76   % 7.20e-03   âš ï¸ (32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 155/400 [45:27<1:26:19, 21.14s/epoch, Loss=0.4613, Val=78.40%, Best=78.76%, Patience=33/40, ETA=1.4h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "155    21.2s    0.4613   78.40   % 78.76   % 7.17e-03   âš ï¸ (33)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 156/400 [45:49<1:27:13, 21.45s/epoch, Loss=0.4580, Val=78.36%, Best=78.76%, Patience=34/40, ETA=1.4h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "156    21.1s    0.4580   78.36   % 78.76   % 7.13e-03   âš ï¸ (34)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 157/400 [46:06<1:21:50, 20.21s/epoch, Loss=0.4595, Val=78.51%, Best=78.76%, Patience=35/40, ETA=1.4h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157    17.3s    0.4595   78.51   % 78.76   % 7.09e-03   âš ï¸ (35)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 158/400 [46:25<1:19:41, 19.76s/epoch, Loss=0.4615, Val=78.69%, Best=78.76%, Patience=36/40, ETA=1.4h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "158    18.7s    0.4615   78.69   % 78.76   % 7.06e-03   âš ï¸ (36)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 159/400 [46:44<1:17:51, 19.39s/epoch, Loss=0.4581, Val=78.44%, Best=78.76%, Patience=37/40, ETA=1.4h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159    18.5s    0.4581   78.44   % 78.76   % 7.02e-03   âš ï¸ (37)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 160/400 [47:02<1:16:30, 19.13s/epoch, Loss=0.4579, Val=78.36%, Best=78.76%, Patience=38/40, ETA=1.3h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160    18.5s    0.4579   78.36   % 78.76   % 6.98e-03   âš ï¸ (38)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 161/400 [47:21<1:15:34, 18.97s/epoch, Loss=0.4567, Val=78.48%, Best=78.76%, Patience=39/40, ETA=1.3h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "161    18.6s    0.4567   78.48   % 78.76   % 6.94e-03   âš ï¸ (39)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Optimized Training:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 161/400 [47:39<1:10:45, 17.76s/epoch, Loss=0.4588, Val=78.61%, Best=78.76%, Patience=40/40, ETA=1.3h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162    18.5s    0.4588   78.61   % 78.76   % 6.90e-03   âš ï¸ (40)\n",
      "\n",
      "â¹ï¸ Early stopping triggered at epoch 162\n",
      "\n",
      "================================================================================\n",
      "ğŸ OPTIMIZED TRAINING COMPLETE!\n",
      "================================================================================\n",
      "ğŸ“Š TRAINING SUMMARY:\n",
      "   Total training time: 47.7m\n",
      "   Average time per epoch: 17.6s\n",
      "   Best epoch: 122\n",
      "   Best accuracy: 0.7876 (78.76%)\n",
      "   Total epochs trained: 162\n",
      "âœ… Loaded best model from epoch 122\n",
      "\n",
      "ğŸ“ˆ IMPROVEMENT ANALYSIS:\n",
      "   Starting baseline: 77.8%\n",
      "   OPTIMIZED result: 78.76%\n",
      "   Net improvement: +0.96 percentage points\n",
      "   Model parameters: 10,380,433 (10.38M)\n",
      "   Parameter efficiency: 0.09 pp/M params\n",
      "\n",
      "ğŸ’ OPTIMIZED achievement: 78.76% with 10.4M parameters!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ’ğŸ¯ FINAL OPTIMIZED STATUS:\n",
      "ğŸ’ Optimized progress: 78.76%\n"
     ]
    }
   ],
   "source": [
    "# ===== RESTART KERNEL FIRST AND RUN THIS COMPLETE CODE =====\n",
    "# OPTIMIZED 5-8M PARAMETER MODEL WITH PROGRESS BAR AND EVERY EPOCH RESULTS\n",
    "\n",
    "# Restart your kernel first, then run this\n",
    "import sys\n",
    "import os\n",
    "import gc\n",
    "import math\n",
    "import warnings\n",
    "import time\n",
    "from datetime import datetime\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Safe torch imports\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.nn.functional as F\n",
    "    print(\"âœ… PyTorch loaded successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"âŒ PyTorch import failed: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Progress bar import\n",
    "try:\n",
    "    from tqdm import tqdm\n",
    "    print(\"âœ… tqdm loaded for progress bars\")\n",
    "except ImportError:\n",
    "    print(\"âš ï¸ tqdm not found, using basic progress\")\n",
    "    class tqdm:\n",
    "        def __init__(self, iterable=None, total=None, desc=\"\", unit=\"epoch\"):\n",
    "            self.iterable = iterable if iterable else range(total)\n",
    "            self.total = total\n",
    "            self.desc = desc\n",
    "            self.current = 0\n",
    "            \n",
    "        def __iter__(self):\n",
    "            return self\n",
    "            \n",
    "        def __next__(self):\n",
    "            if self.current >= len(self.iterable):\n",
    "                raise StopIteration\n",
    "            value = self.iterable[self.current]\n",
    "            self.current += 1\n",
    "            progress = self.current / len(self.iterable) * 100\n",
    "            print(f\"\\r{self.desc} [{self.current}/{len(self.iterable)}] {progress:.1f}%\", end='', flush=True)\n",
    "            return value\n",
    "            \n",
    "        def set_postfix(self, **kwargs):\n",
    "            pass\n",
    "\n",
    "# Safe PyTorch Geometric imports with fallback\n",
    "try:\n",
    "    import torch_geometric\n",
    "    from torch_geometric.nn import HeteroConv, GATConv, SAGEConv\n",
    "    from torch_geometric.nn import Linear\n",
    "    print(\"âœ… PyTorch Geometric loaded successfully\")\n",
    "except (ImportError, AttributeError) as e:\n",
    "    print(f\"âš ï¸ PyTorch Geometric issue: {e}\")\n",
    "    print(\"ğŸ”§ Using fallback Linear layer...\")\n",
    "    Linear = nn.Linear\n",
    "\n",
    "# Check CUDA availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"âœ… Using device: {device}\")\n",
    "\n",
    "class OptimizedGNN58M(nn.Module):\n",
    "    \"\"\"Optimized 5-8M Parameter GNN for Maximum Accuracy\"\"\"\n",
    "    def __init__(self, sequence_features, problem_features, skill_features,\n",
    "                 hidden_dim=320, num_layers=5, num_heads=20, dropout=0.16, device='cuda'):\n",
    "        super(OptimizedGNN58M, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        self.num_heads = num_heads\n",
    "        self.device = device\n",
    "        \n",
    "        # OPTIMIZED input projections - 3x expansion (reduced from 4x)\n",
    "        self.sequence_input_proj = nn.Sequential(\n",
    "            nn.Linear(sequence_features, hidden_dim * 3),  # 3x instead of 4x\n",
    "            nn.BatchNorm1d(hidden_dim * 3),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim * 3, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.GELU()\n",
    "        ).to(device)\n",
    "        \n",
    "        self.problem_input_proj = nn.Sequential(\n",
    "            nn.Linear(problem_features, hidden_dim * 3),\n",
    "            nn.BatchNorm1d(hidden_dim * 3),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim * 3, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.GELU()\n",
    "        ).to(device)\n",
    "        \n",
    "        self.skill_input_proj = nn.Sequential(\n",
    "            nn.Linear(skill_features, hidden_dim * 3),\n",
    "            nn.BatchNorm1d(hidden_dim * 3),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim * 3, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.GELU()\n",
    "        ).to(device)\n",
    "        \n",
    "        # Optimized heterogeneous layers - 5 layers (reduced from 6)\n",
    "        self.conv_layers = nn.ModuleList()\n",
    "        self.batch_norms = nn.ModuleList()\n",
    "        self.residual_projections = nn.ModuleList()\n",
    "        \n",
    "        for i in range(num_layers):\n",
    "            try:\n",
    "                conv_dict = {\n",
    "                    ('sequence', 'predicts', 'problem'): GATConv(\n",
    "                        hidden_dim, hidden_dim // num_heads, heads=num_heads,\n",
    "                        dropout=dropout, add_self_loops=False, edge_dim=9, concat=True\n",
    "                    ),\n",
    "                    ('problem', 'predicted_by', 'sequence'): GATConv(\n",
    "                        hidden_dim, hidden_dim // num_heads, heads=num_heads,\n",
    "                        dropout=dropout, add_self_loops=False, concat=True\n",
    "                    ),\n",
    "                    ('problem', 'requires', 'skill'): SAGEConv(\n",
    "                        (hidden_dim, hidden_dim), hidden_dim, aggr='mean'\n",
    "                    ),\n",
    "                    ('skill', 'required_by', 'problem'): SAGEConv(\n",
    "                        (hidden_dim, hidden_dim), hidden_dim, aggr='mean'\n",
    "                    ),\n",
    "                }\n",
    "                self.conv_layers.append(HeteroConv(conv_dict, aggr='mean').to(device))\n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ Graph layer {i} failed: {e}\")\n",
    "                fallback_layer = nn.Linear(hidden_dim, hidden_dim).to(device)\n",
    "                self.conv_layers.append(fallback_layer)\n",
    "            \n",
    "            bn_dict = nn.ModuleDict({\n",
    "                'sequence': nn.Sequential(nn.BatchNorm1d(hidden_dim), nn.Dropout(dropout)).to(device),\n",
    "                'problem': nn.Sequential(nn.BatchNorm1d(hidden_dim), nn.Dropout(dropout)).to(device),\n",
    "                'skill': nn.Sequential(nn.BatchNorm1d(hidden_dim), nn.Dropout(dropout)).to(device)\n",
    "            })\n",
    "            self.batch_norms.append(bn_dict)\n",
    "            \n",
    "            residual_dict = nn.ModuleDict({\n",
    "                'sequence': nn.Linear(hidden_dim, hidden_dim, bias=False) if i > 0 else nn.Identity(),\n",
    "                'problem': nn.Linear(hidden_dim, hidden_dim, bias=False) if i > 0 else nn.Identity(),\n",
    "                'skill': nn.Linear(hidden_dim, hidden_dim, bias=False) if i > 0 else nn.Identity()\n",
    "            })\n",
    "            self.residual_projections.append(residual_dict.to(device))\n",
    "        \n",
    "        # OPTIMIZED temporal encoder - 4x expansion (reduced from 5x)\n",
    "        self.temporal_encoder = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim * 4),\n",
    "            nn.BatchNorm1d(hidden_dim * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim * 4, hidden_dim * 2),\n",
    "            nn.BatchNorm1d(hidden_dim * 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout * 0.8),\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.GELU()\n",
    "        ).to(device)\n",
    "        \n",
    "        # OPTIMIZED predictor - Reduced depth but maintained capacity\n",
    "        self.predictor = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim * 4),  # 4x expansion\n",
    "            nn.BatchNorm1d(hidden_dim * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout * 2.0),\n",
    "            \n",
    "            nn.Linear(hidden_dim * 4, hidden_dim * 3),  # 3x layer\n",
    "            nn.BatchNorm1d(hidden_dim * 3),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout * 1.6),\n",
    "            \n",
    "            nn.Linear(hidden_dim * 3, hidden_dim * 2),  # 2x layer\n",
    "            nn.BatchNorm1d(hidden_dim * 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout * 1.2),\n",
    "            \n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),  # 1x layer\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            \n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),  # 0.5x layer\n",
    "            nn.BatchNorm1d(hidden_dim // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout * 0.6),\n",
    "            \n",
    "            nn.Linear(hidden_dim // 2, 1)  # Output\n",
    "        ).to(device)\n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.xavier_normal_(module.weight, gain=1.05)  # Slightly reduced gain\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.BatchNorm1d):\n",
    "            nn.init.ones_(module.weight)\n",
    "            nn.init.zeros_(module.bias)\n",
    "    \n",
    "    def safe_hetero_conv(self, conv_layer, x_dict, edge_index_dict, edge_attr_dict=None):\n",
    "        try:\n",
    "            if hasattr(conv_layer, 'convs'):\n",
    "                if edge_attr_dict:\n",
    "                    return conv_layer(x_dict, edge_index_dict, edge_attr_dict)\n",
    "                else:\n",
    "                    return conv_layer(x_dict, edge_index_dict)\n",
    "            else:\n",
    "                return {k: conv_layer(v) for k, v in x_dict.items()}\n",
    "        except Exception as e:\n",
    "            return x_dict\n",
    "    \n",
    "    def forward(self, data):\n",
    "        try:\n",
    "            x_dict = {\n",
    "                'sequence': self.sequence_input_proj(data['sequence'].x.to(self.device)),\n",
    "                'problem': self.problem_input_proj(data['problem'].x.to(self.device)),\n",
    "                'skill': self.skill_input_proj(data['skill'].x.to(self.device))\n",
    "            }\n",
    "            \n",
    "            for i, (conv, bn_dict, res_dict) in enumerate(zip(self.conv_layers, self.batch_norms, self.residual_projections)):\n",
    "                residual = {k: res_dict[k](v) for k, v in x_dict.items()} if i > 0 else None\n",
    "                \n",
    "                if i == 0 and hasattr(data, 'edge_index_dict') and ('sequence', 'predicts', 'problem') in data.edge_index_dict:\n",
    "                    edge_attr_dict = {\n",
    "                        ('sequence', 'predicts', 'problem'): data['sequence', 'predicts', 'problem'].edge_attr.to(self.device)\n",
    "                    }\n",
    "                    x_dict = self.safe_hetero_conv(conv, x_dict, data.edge_index_dict, edge_attr_dict)\n",
    "                else:\n",
    "                    x_dict = self.safe_hetero_conv(conv, x_dict, getattr(data, 'edge_index_dict', {}))\n",
    "                \n",
    "                for node_type in x_dict.keys():\n",
    "                    if node_type in bn_dict:\n",
    "                        x_dict[node_type] = bn_dict[node_type](x_dict[node_type])\n",
    "                        x_dict[node_type] = F.gelu(x_dict[node_type])\n",
    "                        \n",
    "                        if residual is not None and node_type in residual:\n",
    "                            x_dict[node_type] = x_dict[node_type] + 0.15 * residual[node_type]\n",
    "            \n",
    "            sequence_embeddings = self.temporal_encoder(x_dict['sequence'])\n",
    "            problem_embeddings = x_dict['problem']\n",
    "            \n",
    "            if hasattr(data, 'edge_index_dict') and ('sequence', 'predicts', 'problem') in data.edge_index_dict:\n",
    "                edge_index = data['sequence', 'predicts', 'problem'].edge_index.to(self.device)\n",
    "            else:\n",
    "                seq_size = sequence_embeddings.shape[0]\n",
    "                prob_size = problem_embeddings.shape[0]\n",
    "                edge_index = torch.stack([\n",
    "                    torch.arange(min(seq_size, prob_size), device=self.device),\n",
    "                    torch.arange(min(seq_size, prob_size), device=self.device)\n",
    "                ])\n",
    "            \n",
    "            sequence_indices = edge_index[0]\n",
    "            problem_indices = edge_index[1]\n",
    "            \n",
    "            pred_sequence_emb = sequence_embeddings[sequence_indices]\n",
    "            pred_problem_emb = problem_embeddings[problem_indices]\n",
    "            combined_embeddings = torch.cat([pred_sequence_emb, pred_problem_emb], dim=1)\n",
    "            \n",
    "            logits = self.predictor(combined_embeddings).squeeze(-1)\n",
    "            \n",
    "            return logits, {\n",
    "                'sequence_embeddings': sequence_embeddings,\n",
    "                'problem_embeddings': problem_embeddings\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            batch_size = data['sequence'].x.shape[0]\n",
    "            return torch.zeros(batch_size, device=self.device), {}\n",
    "\n",
    "class OptimizedSymbolicEngine58M(nn.Module):\n",
    "    \"\"\"Optimized 5-8M Parameter Symbolic Engine\"\"\"\n",
    "    def __init__(self, device='cuda'):\n",
    "        super(OptimizedSymbolicEngine58M, self).__init__()\n",
    "        self.device = device\n",
    "        \n",
    "        # Enhanced but not excessive rule weights\n",
    "        self.rule_weights = nn.Parameter(torch.tensor([\n",
    "            12.0, 10.5, 9.5, 8.5, 7.5, 6.5, 5.5, 4.5\n",
    "        ], device=device))\n",
    "        \n",
    "        # Optimized thresholds\n",
    "        self.performance_threshold = nn.Parameter(torch.tensor(0.82, device=device))\n",
    "        self.hint_threshold = nn.Parameter(torch.tensor(0.18, device=device))\n",
    "        self.skill_threshold = nn.Parameter(torch.tensor(0.78, device=device))\n",
    "        self.time_optimal = nn.Parameter(torch.tensor(0.62, device=device))\n",
    "        \n",
    "        # OPTIMIZED rule combiner - Reduced size but still powerful\n",
    "        self.rule_combiner = nn.Sequential(\n",
    "            nn.Linear(9, 1024),  # 8 rules + 1 GNN - Reduced from 1536\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.4),\n",
    "            \n",
    "            nn.Linear(1024, 512),  # Reduced from 768\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.35),\n",
    "            \n",
    "            nn.Linear(512, 256),  # Reduced from 384\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            nn.Linear(128, 64),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(64, 1)\n",
    "        ).to(device)\n",
    "        \n",
    "    def safe_extract_features(self, edge_features, gnn_probs):\n",
    "        batch_size = gnn_probs.shape[0]\n",
    "        \n",
    "        try:\n",
    "            hint_usage = torch.clamp(\n",
    "                edge_features[:, 6] if edge_features.shape[1] > 6 \n",
    "                else (1 - gnn_probs) * 0.15, 0, 1\n",
    "            ).to(self.device)\n",
    "            \n",
    "            time_ratio = torch.clamp(\n",
    "                edge_features[:, 5] if edge_features.shape[1] > 5 \n",
    "                else 0.5 + (gnn_probs - 0.5) * 0.15, 0, 2\n",
    "            ).to(self.device)\n",
    "            \n",
    "            skill_overlap = torch.clamp(\n",
    "                edge_features[:, 2] if edge_features.shape[1] > 2 \n",
    "                else gnn_probs * 0.25 + 0.6, 0, 1\n",
    "            ).to(self.device)\n",
    "            \n",
    "            performance_history = torch.clamp(\n",
    "                edge_features[:, 8] if edge_features.shape[1] > 8 \n",
    "                else gnn_probs * 0.98 + 0.01, 0, 1\n",
    "            ).to(self.device)\n",
    "            \n",
    "            recent_trend = torch.clamp(\n",
    "                edge_features[:, 7] if edge_features.shape[1] > 7 \n",
    "                else (gnn_probs - 0.5) * 1.5, -1, 1\n",
    "            ).to(self.device)\n",
    "            \n",
    "        except Exception as e:\n",
    "            hint_usage = (1 - gnn_probs) * 0.25\n",
    "            time_ratio = torch.ones_like(gnn_probs) * 0.5\n",
    "            skill_overlap = gnn_probs * 0.3 + 0.5\n",
    "            performance_history = gnn_probs * 0.85 + 0.1\n",
    "            recent_trend = (gnn_probs - 0.5) * 1.0\n",
    "        \n",
    "        return hint_usage, time_ratio, skill_overlap, performance_history, recent_trend\n",
    "    \n",
    "    def apply_optimized_rules(self, gnn_logits, edge_features):\n",
    "        \"\"\"Apply 8 OPTIMIZED high-impact rules\"\"\"\n",
    "        batch_size = gnn_logits.shape[0]\n",
    "        gnn_probs = torch.sigmoid(gnn_logits)\n",
    "        \n",
    "        hint_usage, time_ratio, skill_overlap, performance_history, recent_trend = self.safe_extract_features(edge_features, gnn_probs)\n",
    "        \n",
    "        confidence_level = torch.abs(gnn_probs - 0.5) * 2\n",
    "        difficulty_level = 1.0 - performance_history\n",
    "        mastery_level = skill_overlap * performance_history\n",
    "        \n",
    "        rules = torch.zeros(batch_size, 8, device=self.device)\n",
    "        \n",
    "        try:\n",
    "            # Rule 1: Performance Consistency - 12x boost\n",
    "            consistency = torch.sigmoid((performance_history - self.performance_threshold) * 45)\n",
    "            confidence_amp = 1 + confidence_level * 1.2\n",
    "            rules[:, 0] = 12.0 * consistency * confidence_amp\n",
    "            \n",
    "            # Rule 2: Skill Mastery Transfer - 10.5x boost\n",
    "            transfer = skill_overlap * performance_history\n",
    "            transfer_boost = torch.sigmoid((transfer - self.skill_threshold) * 40)\n",
    "            neural_synergy = 1 + 0.6 * gnn_probs\n",
    "            rules[:, 1] = 10.5 * transfer_boost * neural_synergy\n",
    "            \n",
    "            # Rule 3: Learning Momentum - 9.5x boost\n",
    "            momentum = torch.sigmoid(recent_trend * 30)\n",
    "            momentum_amp = 1 + 0.5 * performance_history\n",
    "            rules[:, 2] = 9.5 * momentum * momentum_amp\n",
    "            \n",
    "            # Rule 4: Strategic Hint Usage - 8.5x boost\n",
    "            strategic_hints = hint_usage * difficulty_level\n",
    "            appropriate = torch.sigmoid(-(hint_usage - self.hint_threshold) * 60)\n",
    "            rules[:, 3] = 8.5 * strategic_hints * appropriate\n",
    "            \n",
    "            # Rule 5: Time Appropriateness - 7.5x boost\n",
    "            time_optimality = torch.exp(-25 * (time_ratio - self.time_optimal) ** 2)\n",
    "            time_confidence = 1 + 0.4 * confidence_level\n",
    "            rules[:, 4] = 7.5 * time_optimality * time_confidence\n",
    "            \n",
    "            # Rule 6: Difficulty Alignment - 6.5x boost\n",
    "            zpd_alignment = torch.exp(-35 * (difficulty_level - (1 - performance_history)) ** 2)\n",
    "            rules[:, 5] = 6.5 * zpd_alignment\n",
    "            \n",
    "            # Rule 7: Practice Quality - 5.5x boost\n",
    "            practice_quality = torch.clamp(recent_trend, 0, 1) * (1 - hint_usage)\n",
    "            rules[:, 6] = 5.5 * practice_quality\n",
    "            \n",
    "            # Rule 8: Content Familiarity - 4.5x boost\n",
    "            familiarity = skill_overlap * performance_history * (1 + 0.2 * confidence_level)\n",
    "            rules[:, 7] = 4.5 * familiarity\n",
    "            \n",
    "        except Exception as e:\n",
    "            for i in range(8):\n",
    "                rules[:, i] = self.rule_weights[i] * gnn_probs * (i + 1) / 8\n",
    "        \n",
    "        return rules\n",
    "    \n",
    "    def forward(self, gnn_logits, graph_data):\n",
    "        try:\n",
    "            if hasattr(graph_data, 'edge_attr'):\n",
    "                edge_features = graph_data.edge_attr.to(self.device)\n",
    "            elif hasattr(graph_data, '__getitem__') and ('sequence', 'predicts', 'problem') in graph_data:\n",
    "                edge_features = graph_data['sequence', 'predicts', 'problem'].edge_attr.to(self.device)\n",
    "            else:\n",
    "                batch_size = gnn_logits.shape[0]\n",
    "                edge_features = torch.randn(batch_size, 10, device=self.device) * 0.1\n",
    "            \n",
    "            rule_outputs = self.apply_optimized_rules(gnn_logits, edge_features)\n",
    "            weighted_rules = rule_outputs * self.rule_weights.unsqueeze(0)\n",
    "            \n",
    "            gnn_probs = torch.sigmoid(gnn_logits)\n",
    "            combined_input = torch.cat([gnn_probs.unsqueeze(1), weighted_rules], dim=1)\n",
    "            \n",
    "            adjustment = self.rule_combiner(combined_input).squeeze(1)\n",
    "            \n",
    "            # OPTIMIZED scaling - 12x to 25x (reduced from 20x to 50x)\n",
    "            uncertainty = 1 - torch.abs(gnn_probs - 0.5) * 2\n",
    "            confidence_scaling = torch.abs(gnn_probs - 0.5) * 2\n",
    "            adaptive_scale = 12.0 + 10.0 * uncertainty + 6.0 * confidence_scaling\n",
    "            scaled_adjustment = adjustment * adaptive_scale\n",
    "            \n",
    "            adjusted_logits = gnn_logits + scaled_adjustment\n",
    "            \n",
    "            return adjusted_logits, {\n",
    "                'rule_outputs': rule_outputs,\n",
    "                'adjustment': scaled_adjustment,\n",
    "                'scaling_factor': adaptive_scale\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            return gnn_logits, {'rule_outputs': torch.zeros_like(gnn_logits)}\n",
    "\n",
    "class Optimized58MillionModel(nn.Module):\n",
    "    \"\"\"Optimized 5-8 Million Parameter Model\"\"\"\n",
    "    def __init__(self, sequence_features, problem_features, skill_features, device='cuda'):\n",
    "        super(Optimized58MillionModel, self).__init__()\n",
    "        self.device = device\n",
    "        \n",
    "        self.gnn = OptimizedGNN58M(\n",
    "            sequence_features, problem_features, skill_features,\n",
    "            hidden_dim=320, num_layers=5, num_heads=20, dropout=0.16, device=device\n",
    "        )\n",
    "        self.symbolic_engine = OptimizedSymbolicEngine58M(device)\n",
    "        \n",
    "        # Balanced weights\n",
    "        self.neural_weight = nn.Parameter(torch.tensor(0.32, device=device))\n",
    "        self.symbolic_weight = nn.Parameter(torch.tensor(0.68, device=device))\n",
    "        \n",
    "        # Simplified meta combiner\n",
    "        self.meta_combiner = nn.Sequential(\n",
    "            nn.Linear(3, 96),  # Reduced from 128\n",
    "            nn.BatchNorm1d(96),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.15),\n",
    "            nn.Linear(96, 32),  # Reduced from 64\n",
    "            nn.GELU(),\n",
    "            nn.Linear(32, 1),\n",
    "            nn.Tanh()\n",
    "        ).to(device)\n",
    "        \n",
    "    def forward(self, data):\n",
    "        try:\n",
    "            gnn_logits, gnn_info = self.gnn(data)\n",
    "            symbolic_logits, symbolic_info = self.symbolic_engine(gnn_logits, data)\n",
    "            \n",
    "            weights_sum = torch.abs(self.neural_weight) + torch.abs(self.symbolic_weight) + 1e-8\n",
    "            \n",
    "            neural_contrib = (torch.abs(self.neural_weight) / weights_sum) * gnn_logits\n",
    "            symbolic_contrib = (torch.abs(self.symbolic_weight) / weights_sum) * symbolic_logits\n",
    "            \n",
    "            gnn_probs = torch.sigmoid(gnn_logits)\n",
    "            symbolic_probs = torch.sigmoid(symbolic_logits)\n",
    "            confidence = torch.abs(gnn_probs - 0.5) * 2\n",
    "            \n",
    "            if gnn_probs.shape[0] > 1:\n",
    "                meta_input = torch.stack([gnn_probs, symbolic_probs, confidence], dim=1)\n",
    "                meta_weight = self.meta_combiner(meta_input).squeeze(1)\n",
    "                meta_adjustment = meta_weight * (symbolic_logits - gnn_logits)\n",
    "            else:\n",
    "                meta_adjustment = 0.0\n",
    "            \n",
    "            final_logits = neural_contrib + symbolic_contrib + 0.25 * meta_adjustment\n",
    "            \n",
    "            return final_logits, {\n",
    "                'gnn_logits': gnn_logits,\n",
    "                'symbolic_logits': symbolic_logits,\n",
    "                'symbolic_info': symbolic_info,\n",
    "                'neural_weight': torch.abs(self.neural_weight),\n",
    "                'symbolic_weight': torch.abs(self.symbolic_weight)\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            batch_size = data['sequence'].x.shape[0] if 'sequence' in data else 1\n",
    "            return torch.zeros(batch_size, device=self.device), {}\n",
    "\n",
    "def format_time(seconds):\n",
    "    \"\"\"Format seconds into human readable time\"\"\"\n",
    "    if seconds < 60:\n",
    "        return f\"{seconds:.1f}s\"\n",
    "    elif seconds < 3600:\n",
    "        mins = seconds / 60\n",
    "        return f\"{mins:.1f}m\"\n",
    "    else:\n",
    "        hours = seconds / 3600\n",
    "        return f\"{hours:.1f}h\"\n",
    "\n",
    "def optimized_58m_training_with_progress(model_path, device='cuda'):\n",
    "    \"\"\"OPTIMIZED 5-8M Training with PROGRESS BAR and EVERY EPOCH RESULTS\"\"\"\n",
    "    print(\"ğŸš€ğŸ’ OPTIMIZED 5-8M PARAMETER TRAINING WITH PROGRESS BAR ğŸ’ğŸš€\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    try:\n",
    "        # Data loading\n",
    "        print(\"ğŸ“‚ Loading data...\")\n",
    "        start_time = time.time()\n",
    "        saved_data = torch.load(model_path, map_location='cpu')\n",
    "        graph_data = saved_data['graph_data']\n",
    "        targets = saved_data['targets']\n",
    "        \n",
    "        # Device transfer\n",
    "        print(\"ğŸ”„ Transferring to GPU...\")\n",
    "        for key in graph_data.keys():\n",
    "            if hasattr(graph_data[key], 'x') and graph_data[key].x is not None:\n",
    "                graph_data[key].x = graph_data[key].x.to(device)\n",
    "            if hasattr(graph_data[key], 'edge_index') and graph_data[key].edge_index is not None:\n",
    "                graph_data[key].edge_index = graph_data[key].edge_index.to(device)\n",
    "            if hasattr(graph_data[key], 'edge_attr') and graph_data[key].edge_attr is not None:\n",
    "                graph_data[key].edge_attr = graph_data[key].edge_attr.to(device)\n",
    "        \n",
    "        if hasattr(graph_data, 'edge_index_dict'):\n",
    "            edge_index_dict = {}\n",
    "            for edge_type, edge_index in graph_data.edge_index_dict.items():\n",
    "                edge_index_dict[edge_type] = edge_index.to(device)\n",
    "            graph_data.edge_index_dict = edge_index_dict\n",
    "        \n",
    "        targets = targets.to(device)\n",
    "        \n",
    "        # Data split\n",
    "        pos_indices = torch.where(targets == 1)[0]\n",
    "        neg_indices = torch.where(targets == 0)[0]\n",
    "        \n",
    "        pos_perm = torch.randperm(len(pos_indices))\n",
    "        neg_perm = torch.randperm(len(neg_indices))\n",
    "        \n",
    "        pos_train_size = int(0.8 * len(pos_indices))\n",
    "        neg_train_size = int(0.8 * len(neg_indices))\n",
    "        \n",
    "        train_indices = torch.cat([\n",
    "            pos_indices[pos_perm[:pos_train_size]],\n",
    "            neg_indices[neg_perm[:neg_train_size]]\n",
    "        ])\n",
    "        val_indices = torch.cat([\n",
    "            pos_indices[pos_perm[pos_train_size:]],\n",
    "            neg_indices[neg_perm[neg_train_size:]]\n",
    "        ])\n",
    "        \n",
    "        train_targets = targets[train_indices]\n",
    "        val_targets = targets[val_indices]\n",
    "        \n",
    "        print(f\"âœ… Setup complete: {len(train_targets)} train, {len(val_targets)} val\")\n",
    "        print(f\"   Train balance: {train_targets.float().mean():.3f}\")\n",
    "        print(f\"   Val balance: {val_targets.float().mean():.3f}\")\n",
    "        \n",
    "        # Create optimized model\n",
    "        sequence_features = graph_data['sequence'].x.shape[1]\n",
    "        problem_features = graph_data['problem'].x.shape[1]\n",
    "        skill_features = graph_data['skill'].x.shape[1]\n",
    "        \n",
    "        optimized_model = Optimized58MillionModel(sequence_features, problem_features, skill_features, device).to(device)\n",
    "        \n",
    "        total_params = sum(p.numel() for p in optimized_model.parameters())\n",
    "        trainable_params = sum(p.numel() for p in optimized_model.parameters() if p.requires_grad)\n",
    "        \n",
    "        print(f\"ğŸ’ OPTIMIZED Model created:\")\n",
    "        print(f\"   Total parameters: {total_params:,} ({total_params/1e6:.2f}M)\")\n",
    "        print(f\"   Trainable parameters: {trainable_params:,} ({trainable_params/1e6:.2f}M)\")\n",
    "        print(f\"   Target range: 5-8M âœ…\" if 5e6 <= total_params <= 8e6 else f\"   âš ï¸ Outside 5-8M range\")\n",
    "        \n",
    "        # Optimized optimizer\n",
    "        optimizer = torch.optim.AdamW([\n",
    "            {'params': optimized_model.gnn.parameters(), 'lr': 0.0008, 'weight_decay': 8e-5},\n",
    "            {'params': optimized_model.symbolic_engine.parameters(), 'lr': 0.005, 'weight_decay': 4e-5},\n",
    "            {'params': optimized_model.meta_combiner.parameters(), 'lr': 0.003, 'weight_decay': 6e-5},\n",
    "            {'params': [optimized_model.neural_weight, optimized_model.symbolic_weight], 'lr': 0.005}\n",
    "        ])\n",
    "        \n",
    "        criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(1.15, device=device))\n",
    "        scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            optimizer, max_lr=[0.0016, 0.01, 0.006, 0.01], \n",
    "            epochs=400, steps_per_epoch=1, pct_start=0.05, div_factor=4, final_div_factor=80\n",
    "        )\n",
    "        \n",
    "        # Training setup\n",
    "        best_acc = 0.0\n",
    "        best_epoch = 0\n",
    "        patience = 0\n",
    "        max_patience = 40\n",
    "        training_start_time = time.time()\n",
    "        epoch_times = []\n",
    "        \n",
    "        print(\"=\" * 80)\n",
    "        print(\"ğŸš€ STARTING OPTIMIZED TRAINING WITH DETAILED PROGRESS\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"{'Epoch':<6} {'Time':<8} {'Loss':<8} {'Val Acc':<8} {'Best Acc':<9} {'LR':<10} {'Status'}\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        # Training loop with progress bar\n",
    "        progress_bar = tqdm(range(400), desc=\"ğŸš€ Optimized Training\", unit=\"epoch\")\n",
    "        \n",
    "        for epoch in progress_bar:\n",
    "            epoch_start_time = time.time()\n",
    "            optimized_model.train()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            try:\n",
    "                # Forward pass\n",
    "                all_logits, model_info = optimized_model(graph_data)\n",
    "                train_logits = all_logits[train_indices]\n",
    "                \n",
    "                # Loss with enhanced smoothing\n",
    "                smoothed_targets = train_targets.float() * 0.92 + 0.04\n",
    "                loss = criterion(train_logits, smoothed_targets)\n",
    "                \n",
    "                # Backward\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(optimized_model.parameters(), 1.2)\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                \n",
    "                # Current learning rate\n",
    "                current_lr = optimizer.param_groups[1]['lr']\n",
    "                \n",
    "                # Validation EVERY EPOCH\n",
    "                optimized_model.eval()\n",
    "                with torch.no_grad():\n",
    "                    val_logits, _ = optimized_model(graph_data)\n",
    "                    val_probs = torch.sigmoid(val_logits[val_indices])\n",
    "                    \n",
    "                    # Multi-threshold evaluation\n",
    "                    best_threshold_acc = 0\n",
    "                    best_threshold = 0.5\n",
    "                    for threshold in [0.42, 0.45, 0.48, 0.5, 0.52, 0.55, 0.58]:\n",
    "                        acc = ((val_probs > threshold) == val_targets).float().mean().item()\n",
    "                        if acc > best_threshold_acc:\n",
    "                            best_threshold_acc = acc\n",
    "                            best_threshold = threshold\n",
    "                    \n",
    "                    # Track best model\n",
    "                    status = \"\"\n",
    "                    if best_threshold_acc > best_acc + 0.0008:\n",
    "                        best_acc = best_threshold_acc\n",
    "                        best_epoch = epoch\n",
    "                        patience = 0\n",
    "                        torch.save({\n",
    "                            'model_state_dict': optimized_model.state_dict(),\n",
    "                            'epoch': epoch,\n",
    "                            'best_acc': best_acc,\n",
    "                            'threshold': best_threshold,\n",
    "                            'total_params': total_params\n",
    "                        }, 'optimized_58m_model.pt')\n",
    "                        status = \"ğŸ† NEW BEST!\"\n",
    "                    else:\n",
    "                        patience += 1\n",
    "                        if patience <= 5:\n",
    "                            status = f\"ğŸ“ˆ ({patience})\"\n",
    "                        elif patience <= 15:\n",
    "                            status = f\"â³ ({patience})\"\n",
    "                        else:\n",
    "                            status = f\"âš ï¸ ({patience})\"\n",
    "                \n",
    "                # Calculate epoch time\n",
    "                epoch_time = time.time() - epoch_start_time\n",
    "                epoch_times.append(epoch_time)\n",
    "                avg_epoch_time = sum(epoch_times[-10:]) / len(epoch_times[-10:])\n",
    "                \n",
    "                # Estimate remaining time\n",
    "                remaining_epochs = 400 - epoch - 1\n",
    "                estimated_remaining = remaining_epochs * avg_epoch_time\n",
    "                \n",
    "                # Print detailed results\n",
    "                print(f\"{epoch+1:<6} {format_time(epoch_time):<8} {loss.item():<8.4f} \"\n",
    "                      f\"{best_threshold_acc*100:<8.2f}% {best_acc*100:<8.2f}% \"\n",
    "                      f\"{current_lr:<10.2e} {status}\")\n",
    "                \n",
    "                # Update progress bar\n",
    "                progress_bar.set_postfix({\n",
    "                    'Loss': f'{loss.item():.4f}',\n",
    "                    'Val': f'{best_threshold_acc*100:.2f}%',\n",
    "                    'Best': f'{best_acc*100:.2f}%',\n",
    "                    'Patience': f'{patience}/{max_patience}',\n",
    "                    'ETA': format_time(estimated_remaining)\n",
    "                })\n",
    "                \n",
    "                # Success check\n",
    "                if best_acc >= 0.85:\n",
    "                    print(\"\\n\" + \"ğŸ‰\" * 40)\n",
    "                    print(\"ğŸ† 85%+ ACCURACY ACHIEVED! ğŸ†\")\n",
    "                    print(\"ğŸ‰\" * 40)\n",
    "                    break\n",
    "                \n",
    "                # Early stopping\n",
    "                if patience >= max_patience:\n",
    "                    print(f\"\\nâ¹ï¸ Early stopping triggered at epoch {epoch+1}\")\n",
    "                    break\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"\\nâš ï¸ Error at epoch {epoch+1}: {e}\")\n",
    "                torch.cuda.empty_cache()\n",
    "                continue\n",
    "            \n",
    "            # Memory cleanup\n",
    "            if (epoch + 1) % 12 == 0:\n",
    "                torch.cuda.empty_cache()\n",
    "                gc.collect()\n",
    "        \n",
    "        progress_bar.close()\n",
    "        \n",
    "        # Training summary\n",
    "        total_training_time = time.time() - training_start_time\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"ğŸ OPTIMIZED TRAINING COMPLETE!\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"ğŸ“Š TRAINING SUMMARY:\")\n",
    "        print(f\"   Total training time: {format_time(total_training_time)}\")\n",
    "        print(f\"   Average time per epoch: {format_time(sum(epoch_times)/len(epoch_times))}\")\n",
    "        print(f\"   Best epoch: {best_epoch + 1}\")\n",
    "        print(f\"   Best accuracy: {best_acc:.4f} ({best_acc*100:.2f}%)\")\n",
    "        print(f\"   Total epochs trained: {len(epoch_times)}\")\n",
    "        \n",
    "        # Load best model\n",
    "        try:\n",
    "            checkpoint = torch.load('optimized_58m_model.pt')\n",
    "            optimized_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            print(f\"âœ… Loaded best model from epoch {checkpoint['epoch']+1}\")\n",
    "        except:\n",
    "            print(\"âš ï¸ Using final model state\")\n",
    "        \n",
    "        improvement = (best_acc - 0.778) * 100\n",
    "        \n",
    "        print(f\"\\nğŸ“ˆ IMPROVEMENT ANALYSIS:\")\n",
    "        print(f\"   Starting baseline: 77.8%\")\n",
    "        print(f\"   OPTIMIZED result: {best_acc*100:.2f}%\")\n",
    "        print(f\"   Net improvement: +{improvement:.2f} percentage points\")\n",
    "        print(f\"   Model parameters: {total_params:,} ({total_params/1e6:.2f}M)\")\n",
    "        print(f\"   Parameter efficiency: {improvement/(total_params/1e6):.2f} pp/M params\")\n",
    "        \n",
    "        if best_acc >= 0.85:\n",
    "            print(\"\\nğŸ‰ğŸ’ğŸ† OPTIMIZED SUCCESS! 85%+ TARGET ACHIEVED! ğŸ†ğŸ’ğŸ‰\")\n",
    "        elif best_acc >= 0.82:\n",
    "            print(\"\\nğŸŠğŸ’ OPTIMIZED EXCELLENCE! Very close to 85%!\")\n",
    "        else:\n",
    "            print(f\"\\nğŸ’ OPTIMIZED achievement: {best_acc*100:.2f}% with {total_params/1e6:.1f}M parameters!\")\n",
    "        \n",
    "        return optimized_model, best_acc\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Training failed: {e}\")\n",
    "        return None, 0.0\n",
    "\n",
    "# ===== LAUNCH OPTIMIZED TRAINING WITH PROGRESS =====\n",
    "print(\"ğŸš€ğŸ’ LAUNCHING OPTIMIZED 5-8M PARAMETER TRAINING ğŸ’ğŸš€\")\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "# Run training\n",
    "try:\n",
    "    optimized_model, optimized_accuracy = optimized_58m_training_with_progress(\n",
    "        model_path='./teg_nesynet_models/teg_nesynet_temporal_v1.pt',\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nğŸ’ğŸ¯ FINAL OPTIMIZED STATUS:\")\n",
    "    if optimized_accuracy >= 0.85:\n",
    "        print(f\"ğŸ† OPTIMIZED SUCCESS! {optimized_accuracy*100:.2f}% ACHIEVED!\")\n",
    "    else:\n",
    "        print(f\"ğŸ’ Optimized progress: {optimized_accuracy*100:.2f}%\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Optimization failed: {e}\")\n",
    "    print(\"ğŸ”§ Please restart kernel and try again\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9befc73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ EVALUATING EXISTING TRAINED MODEL\n",
      "âœ… Using device: cuda\n",
      "ğŸ“Š COMPREHENSIVE MODEL EVALUATION FOR RESEARCH PAPER\n",
      "======================================================================\n",
      "ğŸ“‚ Loading graph data...\n",
      "ğŸ”„ Transferring to GPU...\n",
      "âœ… Validation set: 4764 samples\n",
      "ğŸ”„ Loading trained model...\n",
      "âŒ Evaluation failed: name 'Optimized58MillionModel' is not defined\n"
     ]
    }
   ],
   "source": [
    "# Restart your kernel first, then run this\n",
    "import sys\n",
    "import os\n",
    "import gc\n",
    "import math\n",
    "import warnings\n",
    "import time\n",
    "import torch\n",
    "from datetime import datetime\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def comprehensive_model_evaluation(model_path, saved_model_path, device='cuda'):\n",
    "    \"\"\"Load trained model and generate comprehensive evaluation for research paper\"\"\"\n",
    "    \n",
    "    print(\"ğŸ“Š COMPREHENSIVE MODEL EVALUATION FOR RESEARCH PAPER\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    try:\n",
    "        # Load data\n",
    "        print(\"ğŸ“‚ Loading graph data...\")\n",
    "        saved_data = torch.load(model_path, map_location='cpu')\n",
    "        graph_data = saved_data['graph_data']\n",
    "        targets = saved_data['targets']\n",
    "        \n",
    "        # Transfer to GPU\n",
    "        print(\"ğŸ”„ Transferring to GPU...\")\n",
    "        for key in graph_data.keys():\n",
    "            if hasattr(graph_data[key], 'x') and graph_data[key].x is not None:\n",
    "                graph_data[key].x = graph_data[key].x.to(device)\n",
    "            if hasattr(graph_data[key], 'edge_index') and graph_data[key].edge_index is not None:\n",
    "                graph_data[key].edge_index = graph_data[key].edge_index.to(device)\n",
    "            if hasattr(graph_data[key], 'edge_attr') and graph_data[key].edge_attr is not None:\n",
    "                graph_data[key].edge_attr = graph_data[key].edge_attr.to(device)\n",
    "        \n",
    "        if hasattr(graph_data, 'edge_index_dict'):\n",
    "            edge_index_dict = {}\n",
    "            for edge_type, edge_index in graph_data.edge_index_dict.items():\n",
    "                edge_index_dict[edge_type] = edge_index.to(device)\n",
    "            graph_data.edge_index_dict = edge_index_dict\n",
    "        \n",
    "        targets = targets.to(device)\n",
    "        \n",
    "        # Create validation split (same as training)\n",
    "        pos_indices = torch.where(targets == 1)[0]\n",
    "        neg_indices = torch.where(targets == 0)[0]\n",
    "        \n",
    "        pos_perm = torch.randperm(len(pos_indices))\n",
    "        neg_perm = torch.randperm(len(neg_indices))\n",
    "        \n",
    "        pos_train_size = int(0.8 * len(pos_indices))\n",
    "        neg_train_size = int(0.8 * len(neg_indices))\n",
    "        \n",
    "        val_indices = torch.cat([\n",
    "            pos_indices[pos_perm[pos_train_size:]],\n",
    "            neg_indices[neg_perm[neg_train_size:]]\n",
    "        ])\n",
    "        val_targets = targets[val_indices]\n",
    "        \n",
    "        print(f\"âœ… Validation set: {len(val_targets)} samples\")\n",
    "        \n",
    "        # Recreate model architecture\n",
    "        sequence_features = graph_data['sequence'].x.shape[1]\n",
    "        problem_features = graph_data['problem'].x.shape[1]\n",
    "        skill_features = graph_data['skill'].x.shape[1]\n",
    "        \n",
    "        # Load your trained model\n",
    "        print(\"ğŸ”„ Loading trained model...\")\n",
    "        optimized_model = Optimized58MillionModel(sequence_features, problem_features, skill_features, device).to(device)\n",
    "        \n",
    "        # Load the trained weights\n",
    "        checkpoint = torch.load(saved_model_path, map_location=device)\n",
    "        optimized_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        \n",
    "        total_params = sum(p.numel() for p in optimized_model.parameters())\n",
    "        print(f\"âœ… Model loaded: {total_params:,} parameters\")\n",
    "        \n",
    "        # COMPREHENSIVE EVALUATION\n",
    "        print(\"ğŸ“Š Running comprehensive evaluation...\")\n",
    "        optimized_model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            val_logits, model_info = optimized_model(graph_data)\n",
    "            final_predictions = torch.sigmoid(val_logits[val_indices]).cpu().numpy()\n",
    "            final_targets = val_targets.cpu().numpy()\n",
    "        \n",
    "        # Calculate all metrics\n",
    "        from sklearn.metrics import (\n",
    "            confusion_matrix, roc_curve, auc, precision_recall_curve,\n",
    "            accuracy_score, precision_score, recall_score, f1_score\n",
    "        )\n",
    "        \n",
    "        pred_binary = (final_predictions > 0.5).astype(int)\n",
    "        \n",
    "        final_accuracy = accuracy_score(final_targets, pred_binary)\n",
    "        final_precision = precision_score(final_targets, pred_binary, zero_division=0)\n",
    "        final_recall = recall_score(final_targets, pred_binary, zero_division=0)\n",
    "        final_f1 = f1_score(final_targets, pred_binary, zero_division=0)\n",
    "        \n",
    "        cm = confusion_matrix(final_targets, pred_binary)\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "        sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        \n",
    "        # Calculate ROC and PR curves\n",
    "        fpr, tpr, _ = roc_curve(final_targets, final_predictions)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        precision_curve, recall_curve, _ = precision_recall_curve(final_targets, final_predictions)\n",
    "        \n",
    "        print(\"ğŸ“Š RESULTS:\")\n",
    "        print(f\"   Accuracy: {final_accuracy:.4f} ({final_accuracy*100:.2f}%)\")\n",
    "        print(f\"   Precision: {final_precision:.4f}\")\n",
    "        print(f\"   Recall: {final_recall:.4f}\")\n",
    "        print(f\"   F1-Score: {final_f1:.4f}\")\n",
    "        print(f\"   AUC: {roc_auc:.4f}\")\n",
    "        \n",
    "        # Create comprehensive results\n",
    "        comprehensive_results = {\n",
    "            'final_metrics': {\n",
    "                'accuracy': final_accuracy,\n",
    "                'precision': final_precision,\n",
    "                'recall': final_recall,\n",
    "                'f1': final_f1,\n",
    "                'auc': roc_auc,\n",
    "                'specificity': specificity,\n",
    "                'sensitivity': sensitivity\n",
    "            },\n",
    "            'confusion_matrix': cm.tolist(),\n",
    "            'roc_data': {\n",
    "                'fpr': fpr.tolist(),\n",
    "                'tpr': tpr.tolist(),\n",
    "                'auc': roc_auc\n",
    "            },\n",
    "            'pr_data': {\n",
    "                'precision': precision_curve.tolist(),\n",
    "                'recall': recall_curve.tolist()\n",
    "            },\n",
    "            'model_info': {\n",
    "                'name': 'GNN+Neurosymbolic',\n",
    "                'parameters': f'{total_params:,}',\n",
    "                'architecture': 'Heterogeneous GNN + Symbolic Rules'\n",
    "            },\n",
    "            'predictions': final_predictions.tolist(),\n",
    "            'targets': final_targets.tolist()\n",
    "        }\n",
    "        \n",
    "        # Save comprehensive results\n",
    "        torch.save(comprehensive_results, 'gnn_neurosymbolic_comprehensive_evaluation.pth')\n",
    "        \n",
    "        # Generate publication-quality visualizations\n",
    "        print(\"ğŸ“Š Generating visualizations...\")\n",
    "        import matplotlib.pyplot as plt\n",
    "        import seaborn as sns\n",
    "        \n",
    "        plt.style.use('default')\n",
    "        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
    "        \n",
    "        # Model performance comparison (if you have other models' results)\n",
    "        models = ['LSTM\\n(2.1M)', 'Transformer\\n(3.8M)', 'GNN+Neuro\\n(10.4M)']\n",
    "        accuracies = [0.746, 0.769, final_accuracy]  # Update with actual values\n",
    "        \n",
    "        ax1.bar(models, accuracies, color=['blue', 'green', 'red'], alpha=0.7)\n",
    "        ax1.set_ylabel('Accuracy')\n",
    "        ax1.set_title('Model Performance Comparison')\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        for i, v in enumerate(accuracies):\n",
    "            ax1.text(i, v + 0.005, f'{v:.3f}', ha='center', va='bottom')\n",
    "        \n",
    "        # ROC Curve\n",
    "        ax2.plot(fpr, tpr, color='red', lw=2, label=f'GNN+Neuro (AUC = {roc_auc:.3f})')\n",
    "        ax2.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', alpha=0.6)\n",
    "        ax2.set_xlabel('False Positive Rate')\n",
    "        ax2.set_ylabel('True Positive Rate')\n",
    "        ax2.set_title('ROC Curve')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Precision-Recall Curve\n",
    "        ax3.plot(recall_curve, precision_curve, color='green', lw=2)\n",
    "        ax3.set_xlabel('Recall')\n",
    "        ax3.set_ylabel('Precision')\n",
    "        ax3.set_title('Precision-Recall Curve')\n",
    "        ax3.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Confusion Matrix\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Reds', ax=ax4,\n",
    "                    xticklabels=['Incorrect', 'Correct'],\n",
    "                    yticklabels=['Incorrect', 'Correct'])\n",
    "        ax4.set_title('Confusion Matrix')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('gnn_neurosymbolic_comprehensive_evaluation.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"âœ… Comprehensive evaluation completed!\")\n",
    "        print(\"   Files generated:\")\n",
    "        print(\"   - gnn_neurosymbolic_comprehensive_evaluation.pth\")\n",
    "        print(\"   - gnn_neurosymbolic_comprehensive_evaluation.png\")\n",
    "        \n",
    "        return comprehensive_results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Evaluation failed: {e}\")\n",
    "        return None\n",
    "\n",
    "# ===== RUN EVALUATION ON YOUR EXISTING TRAINED MODEL =====\n",
    "print(\"ğŸš€ EVALUATING EXISTING TRAINED MODEL\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"âœ… Using device: {device}\")\n",
    "# Run evaluation using your existing files\n",
    "results = comprehensive_model_evaluation(\n",
    "    model_path='./teg_nesynet_models/teg_nesynet_temporal_v1.pt',  # Your graph data\n",
    "    saved_model_path='optimized_58m_model.pt',  # Your trained model\n",
    "    device=device\n",
    ")\n",
    "\n",
    "if results:\n",
    "    print(f\"\\nğŸ¯ FINAL EVALUATION COMPLETE!\")\n",
    "    print(f\"   Your model achieved: {results['final_metrics']['accuracy']*100:.2f}% accuracy\")\n",
    "    print(\"   Ready for research paper! ğŸ“\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
