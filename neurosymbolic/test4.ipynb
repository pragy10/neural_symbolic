{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8729af5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ PyTorch loaded successfully\n",
      "✅ tqdm loaded for progress bars\n",
      "✅ PyTorch Geometric loaded successfully\n",
      "✅ Using device: cuda\n",
      "🚀💎 LAUNCHING OPTIMIZED 5-8M PARAMETER TRAINING 💎🚀\n",
      "🚀💎 OPTIMIZED 5-8M PARAMETER TRAINING WITH PROGRESS BAR 💎🚀\n",
      "================================================================================\n",
      "📂 Loading data...\n",
      "🔄 Transferring to GPU...\n",
      "✅ Setup complete: 19056 train, 4764 val\n",
      "   Train balance: 0.543\n",
      "   Val balance: 0.543\n",
      "💎 OPTIMIZED Model created:\n",
      "   Total parameters: 10,380,433 (10.38M)\n",
      "   Trainable parameters: 10,380,433 (10.38M)\n",
      "   ⚠️ Outside 5-8M range\n",
      "================================================================================\n",
      "🚀 STARTING OPTIMIZED TRAINING WITH DETAILED PROGRESS\n",
      "================================================================================\n",
      "Epoch  Time     Loss     Val Acc  Best Acc  LR         Status\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:   0%|          | 1/400 [00:23<2:37:03, 23.62s/epoch, Loss=1.4700, Val=45.74%, Best=45.74%, Patience=0/40, ETA=2.6h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1      23.6s    1.4700   45.74   % 45.74   % 2.55e-03   🏆 NEW BEST!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:   0%|          | 2/400 [00:47<2:39:34, 24.06s/epoch, Loss=1.8093, Val=54.26%, Best=54.26%, Patience=0/40, ETA=2.7h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2      24.4s    1.8093   54.26   % 54.26   % 2.70e-03   🏆 NEW BEST!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:   1%|          | 3/400 [01:13<2:43:09, 24.66s/epoch, Loss=1.6239, Val=54.26%, Best=54.26%, Patience=1/40, ETA=2.7h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3      25.4s    1.6239   54.26   % 54.26   % 2.95e-03   📈 (1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:   1%|          | 4/400 [01:39<2:45:31, 25.08s/epoch, Loss=1.0538, Val=49.52%, Best=54.26%, Patience=2/40, ETA=2.7h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4      25.7s    1.0538   49.52   % 54.26   % 3.29e-03   📈 (2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:   1%|▏         | 5/400 [02:04<2:47:04, 25.38s/epoch, Loss=1.2799, Val=45.74%, Best=54.26%, Patience=3/40, ETA=2.7h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5      25.9s    1.2799   45.74   % 54.26   % 3.71e-03   📈 (3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:   2%|▏         | 6/400 [02:31<2:48:11, 25.61s/epoch, Loss=1.1168, Val=53.40%, Best=54.26%, Patience=4/40, ETA=2.8h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6      26.1s    1.1168   53.40   % 54.26   % 4.20e-03   📈 (4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:   2%|▏         | 7/400 [02:56<2:47:59, 25.65s/epoch, Loss=0.8587, Val=51.66%, Best=54.26%, Patience=5/40, ETA=2.8h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7      25.7s    0.8587   51.66   % 54.26   % 4.74e-03   📈 (5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:   2%|▏         | 8/400 [03:22<2:47:45, 25.68s/epoch, Loss=0.8430, Val=48.17%, Best=54.26%, Patience=6/40, ETA=2.8h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8      25.7s    0.8430   48.17   % 54.26   % 5.33e-03   ⏳ (6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:   2%|▏         | 9/400 [03:48<2:48:11, 25.81s/epoch, Loss=0.7998, Val=53.95%, Best=54.26%, Patience=7/40, ETA=2.8h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9      26.1s    0.7998   53.95   % 54.26   % 5.94e-03   ⏳ (7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:   2%|▎         | 10/400 [04:14<2:48:03, 25.86s/epoch, Loss=0.7838, Val=48.55%, Best=54.26%, Patience=8/40, ETA=2.8h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10     26.0s    0.7838   48.55   % 54.26   % 6.56e-03   ⏳ (8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:   3%|▎         | 11/400 [04:40<2:47:15, 25.80s/epoch, Loss=0.7677, Val=50.06%, Best=54.26%, Patience=9/40, ETA=2.8h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11     25.7s    0.7677   50.06   % 54.26   % 7.17e-03   ⏳ (9)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:   3%|▎         | 11/400 [05:05<2:47:15, 25.80s/epoch, Loss=0.7345, Val=51.87%, Best=54.26%, Patience=10/40, ETA=2.8h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12     25.5s    0.7345   51.87   % 54.26   % 7.76e-03   ⏳ (10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:   3%|▎         | 13/400 [05:24<2:32:17, 23.61s/epoch, Loss=0.7132, Val=54.26%, Best=54.26%, Patience=11/40, ETA=2.7h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13     18.2s    0.7132   54.26   % 54.26   % 8.30e-03   ⏳ (11)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:   4%|▎         | 14/400 [05:45<2:25:53, 22.68s/epoch, Loss=0.7033, Val=54.35%, Best=54.35%, Patience=0/40, ETA=2.6h] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14     20.5s    0.7033   54.35   % 54.35   % 8.79e-03   🏆 NEW BEST!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:   4%|▍         | 15/400 [06:05<2:19:48, 21.79s/epoch, Loss=0.6911, Val=54.26%, Best=54.35%, Patience=1/40, ETA=2.6h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15     19.7s    0.6911   54.26   % 54.35   % 9.21e-03   📈 (1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:   4%|▍         | 16/400 [06:24<2:15:20, 21.15s/epoch, Loss=0.6942, Val=54.26%, Best=54.35%, Patience=2/40, ETA=2.5h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16     19.7s    0.6942   54.26   % 54.35   % 9.55e-03   📈 (2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:   4%|▍         | 17/400 [06:44<2:11:54, 20.66s/epoch, Loss=0.6832, Val=54.28%, Best=54.35%, Patience=3/40, ETA=2.4h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17     19.5s    0.6832   54.28   % 54.35   % 9.80e-03   📈 (3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:   4%|▍         | 18/400 [07:03<2:09:05, 20.28s/epoch, Loss=0.6808, Val=54.26%, Best=54.35%, Patience=4/40, ETA=2.3h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18     19.4s    0.6808   54.26   % 54.35   % 9.95e-03   📈 (4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:   5%|▍         | 19/400 [07:23<2:08:17, 20.20s/epoch, Loss=0.6729, Val=54.30%, Best=54.35%, Patience=5/40, ETA=2.3h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19     20.0s    0.6729   54.30   % 54.35   % 1.00e-02   📈 (5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:   5%|▌         | 20/400 [07:43<2:07:17, 20.10s/epoch, Loss=0.6683, Val=54.37%, Best=54.35%, Patience=6/40, ETA=2.2h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20     19.8s    0.6683   54.37   % 54.35   % 1.00e-02   ⏳ (6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:   5%|▌         | 21/400 [08:03<2:06:27, 20.02s/epoch, Loss=0.6605, Val=55.79%, Best=55.79%, Patience=0/40, ETA=2.1h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21     19.8s    0.6605   55.79   % 55.79   % 1.00e-02   🏆 NEW BEST!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:   6%|▌         | 22/400 [08:22<2:05:21, 19.90s/epoch, Loss=0.6538, Val=55.67%, Best=55.79%, Patience=1/40, ETA=2.1h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22     19.6s    0.6538   55.67   % 55.79   % 1.00e-02   📈 (1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:   6%|▌         | 23/400 [08:43<2:05:35, 19.99s/epoch, Loss=0.6479, Val=55.81%, Best=55.79%, Patience=2/40, ETA=2.1h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23     20.2s    0.6479   55.81   % 55.79   % 1.00e-02   📈 (2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:   6%|▌         | 23/400 [09:03<2:05:35, 19.99s/epoch, Loss=0.6442, Val=57.12%, Best=57.12%, Patience=0/40, ETA=2.1h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24     20.1s    0.6442   57.12   % 57.12   % 1.00e-02   🏆 NEW BEST!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:   6%|▋         | 25/400 [09:22<2:03:38, 19.78s/epoch, Loss=0.6377, Val=57.37%, Best=57.37%, Patience=0/40, ETA=2.0h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25     18.6s    0.6377   57.37   % 57.37   % 9.99e-03   🏆 NEW BEST!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:   6%|▋         | 26/400 [09:42<2:03:43, 19.85s/epoch, Loss=0.6357, Val=57.70%, Best=57.70%, Patience=0/40, ETA=2.0h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26     20.0s    0.6357   57.70   % 57.70   % 9.99e-03   🏆 NEW BEST!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:   7%|▋         | 27/400 [10:03<2:05:28, 20.18s/epoch, Loss=0.6347, Val=57.79%, Best=57.79%, Patience=0/40, ETA=2.1h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27     21.0s    0.6347   57.79   % 57.79   % 9.99e-03   🏆 NEW BEST!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:   7%|▋         | 28/400 [10:24<2:05:42, 20.28s/epoch, Loss=0.6262, Val=58.73%, Best=58.73%, Patience=0/40, ETA=2.1h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28     20.5s    0.6262   58.73   % 58.73   % 9.99e-03   🏆 NEW BEST!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:   7%|▋         | 29/400 [10:44<2:05:09, 20.24s/epoch, Loss=0.6289, Val=58.75%, Best=58.73%, Patience=1/40, ETA=2.1h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29     20.2s    0.6289   58.75   % 58.73   % 9.98e-03   📈 (1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:   8%|▊         | 30/400 [11:04<2:03:52, 20.09s/epoch, Loss=0.6156, Val=59.66%, Best=59.66%, Patience=0/40, ETA=2.1h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30     19.7s    0.6156   59.66   % 59.66   % 9.98e-03   🏆 NEW BEST!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:   8%|▊         | 31/400 [11:23<2:02:28, 19.92s/epoch, Loss=0.6156, Val=59.24%, Best=59.66%, Patience=1/40, ETA=2.0h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31     19.5s    0.6156   59.24   % 59.66   % 9.98e-03   📈 (1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:   8%|▊         | 32/400 [11:43<2:02:17, 19.94s/epoch, Loss=0.6170, Val=60.31%, Best=60.31%, Patience=0/40, ETA=2.0h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32     20.0s    0.6170   60.31   % 60.31   % 9.97e-03   🏆 NEW BEST!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:   8%|▊         | 33/400 [12:03<2:01:54, 19.93s/epoch, Loss=0.6304, Val=61.25%, Best=61.25%, Patience=0/40, ETA=2.0h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33     19.9s    0.6304   61.25   % 61.25   % 9.97e-03   🏆 NEW BEST!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:   8%|▊         | 34/400 [12:23<2:02:14, 20.04s/epoch, Loss=0.6189, Val=62.87%, Best=62.87%, Patience=0/40, ETA=2.0h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34     20.3s    0.6189   62.87   % 62.87   % 9.96e-03   🏆 NEW BEST!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:   9%|▉         | 35/400 [12:43<2:01:50, 20.03s/epoch, Loss=0.6182, Val=63.79%, Best=63.79%, Patience=0/40, ETA=2.0h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35     20.0s    0.6182   63.79   % 63.79   % 9.96e-03   🏆 NEW BEST!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:   9%|▉         | 36/400 [13:04<2:02:03, 20.12s/epoch, Loss=0.6096, Val=63.20%, Best=63.79%, Patience=1/40, ETA=2.0h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36     19.7s    0.6096   63.20   % 63.79   % 9.95e-03   📈 (1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:   9%|▉         | 37/400 [13:22<1:59:09, 19.70s/epoch, Loss=0.6030, Val=62.47%, Best=63.79%, Patience=2/40, ETA=2.0h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37     18.7s    0.6030   62.47   % 63.79   % 9.94e-03   📈 (2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:  10%|▉         | 38/400 [13:41<1:57:24, 19.46s/epoch, Loss=0.5920, Val=63.01%, Best=63.79%, Patience=3/40, ETA=2.0h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38     18.9s    0.5920   63.01   % 63.79   % 9.94e-03   📈 (3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:  10%|▉         | 39/400 [14:00<1:56:39, 19.39s/epoch, Loss=0.5953, Val=62.47%, Best=63.79%, Patience=4/40, ETA=2.0h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39     19.2s    0.5953   62.47   % 63.79   % 9.93e-03   📈 (4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:  10%|█         | 40/400 [14:20<1:55:56, 19.32s/epoch, Loss=0.5851, Val=64.78%, Best=64.78%, Patience=0/40, ETA=2.0h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40     19.2s    0.5851   64.78   % 64.78   % 9.93e-03   🏆 NEW BEST!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:  10%|█         | 41/400 [14:39<1:55:31, 19.31s/epoch, Loss=0.5813, Val=66.23%, Best=66.23%, Patience=0/40, ETA=1.9h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41     19.3s    0.5813   66.23   % 66.23   % 9.92e-03   🏆 NEW BEST!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:  10%|█         | 42/400 [14:58<1:54:57, 19.27s/epoch, Loss=0.5794, Val=66.67%, Best=66.67%, Patience=0/40, ETA=1.9h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42     19.2s    0.5794   66.67   % 66.67   % 9.91e-03   🏆 NEW BEST!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:  11%|█         | 43/400 [15:17<1:54:38, 19.27s/epoch, Loss=0.5773, Val=67.49%, Best=67.49%, Patience=0/40, ETA=1.9h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43     19.3s    0.5773   67.49   % 67.49   % 9.90e-03   🏆 NEW BEST!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:  11%|█         | 44/400 [15:36<1:54:01, 19.22s/epoch, Loss=0.5682, Val=68.09%, Best=68.09%, Patience=0/40, ETA=1.9h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44     19.1s    0.5682   68.09   % 68.09   % 9.89e-03   🏆 NEW BEST!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:  11%|█▏        | 45/400 [15:56<1:53:39, 19.21s/epoch, Loss=0.5667, Val=69.82%, Best=69.82%, Patience=0/40, ETA=1.9h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45     19.2s    0.5667   69.82   % 69.82   % 9.89e-03   🏆 NEW BEST!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:  12%|█▏        | 46/400 [16:15<1:53:13, 19.19s/epoch, Loss=0.5639, Val=70.00%, Best=70.00%, Patience=0/40, ETA=1.9h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46     19.1s    0.5639   70.00   % 70.00   % 9.88e-03   🏆 NEW BEST!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:  12%|█▏        | 47/400 [16:34<1:52:39, 19.15s/epoch, Loss=0.5562, Val=69.27%, Best=70.00%, Patience=1/40, ETA=1.9h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47     19.0s    0.5562   69.27   % 70.00   % 9.87e-03   📈 (1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:  12%|█▏        | 47/400 [16:53<1:52:39, 19.15s/epoch, Loss=0.5594, Val=71.47%, Best=71.47%, Patience=0/40, ETA=1.9h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48     19.1s    0.5594   71.47   % 71.47   % 9.86e-03   🏆 NEW BEST!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:  12%|█▏        | 49/400 [17:14<1:54:01, 19.49s/epoch, Loss=0.5622, Val=71.28%, Best=71.47%, Patience=1/40, ETA=1.9h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49     19.7s    0.5622   71.28   % 71.47   % 9.85e-03   📈 (1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:  12%|█▎        | 50/400 [17:32<1:51:35, 19.13s/epoch, Loss=0.5550, Val=72.61%, Best=72.61%, Patience=0/40, ETA=1.9h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50     18.3s    0.5550   72.61   % 72.61   % 9.84e-03   🏆 NEW BEST!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:  13%|█▎        | 51/400 [17:52<1:53:20, 19.49s/epoch, Loss=0.5492, Val=73.07%, Best=73.07%, Patience=0/40, ETA=1.9h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51     20.3s    0.5492   73.07   % 73.07   % 9.83e-03   🏆 NEW BEST!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:  13%|█▎        | 52/400 [18:12<1:54:07, 19.68s/epoch, Loss=0.5466, Val=72.65%, Best=73.07%, Patience=1/40, ETA=1.9h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52     20.1s    0.5466   72.65   % 73.07   % 9.82e-03   📈 (1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:  13%|█▎        | 53/400 [18:32<1:54:37, 19.82s/epoch, Loss=0.5480, Val=72.94%, Best=73.07%, Patience=2/40, ETA=1.9h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53     20.1s    0.5480   72.94   % 73.07   % 9.80e-03   📈 (2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:  14%|█▎        | 54/400 [18:53<1:54:48, 19.91s/epoch, Loss=0.5432, Val=72.67%, Best=73.07%, Patience=3/40, ETA=1.9h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54     20.1s    0.5432   72.67   % 73.07   % 9.79e-03   📈 (3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:  14%|█▍        | 55/400 [19:13<1:54:45, 19.96s/epoch, Loss=0.5355, Val=72.29%, Best=73.07%, Patience=4/40, ETA=1.9h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55     20.1s    0.5355   72.29   % 73.07   % 9.78e-03   📈 (4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:  14%|█▍        | 56/400 [19:33<1:54:50, 20.03s/epoch, Loss=0.5355, Val=73.59%, Best=73.59%, Patience=0/40, ETA=1.9h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56     20.2s    0.5355   73.59   % 73.59   % 9.77e-03   🏆 NEW BEST!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:  14%|█▍        | 57/400 [19:53<1:54:39, 20.06s/epoch, Loss=0.5361, Val=73.13%, Best=73.59%, Patience=1/40, ETA=1.9h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57     20.1s    0.5361   73.13   % 73.59   % 9.76e-03   📈 (1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:  14%|█▍        | 58/400 [20:13<1:54:27, 20.08s/epoch, Loss=0.5336, Val=72.90%, Best=73.59%, Patience=2/40, ETA=1.9h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58     20.1s    0.5336   72.90   % 73.59   % 9.74e-03   📈 (2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:  15%|█▍        | 59/400 [20:34<1:55:12, 20.27s/epoch, Loss=0.5305, Val=74.29%, Best=74.29%, Patience=0/40, ETA=1.9h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59     20.7s    0.5305   74.29   % 74.29   % 9.73e-03   🏆 NEW BEST!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:  15%|█▍        | 59/400 [20:54<1:55:12, 20.27s/epoch, Loss=0.5262, Val=74.75%, Best=74.75%, Patience=0/40, ETA=1.9h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60     20.3s    0.5262   74.75   % 74.75   % 9.72e-03   🏆 NEW BEST!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:  15%|█▌        | 61/400 [21:13<1:51:14, 19.69s/epoch, Loss=0.5223, Val=74.64%, Best=74.75%, Patience=1/40, ETA=1.9h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61     17.8s    0.5223   74.64   % 74.75   % 9.70e-03   📈 (1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:  16%|█▌        | 62/400 [21:32<1:49:52, 19.50s/epoch, Loss=0.5204, Val=74.90%, Best=74.90%, Patience=0/40, ETA=1.9h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62     19.1s    0.5204   74.90   % 74.90   % 9.69e-03   🏆 NEW BEST!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:  16%|█▌        | 63/400 [21:51<1:48:55, 19.39s/epoch, Loss=0.5171, Val=75.21%, Best=75.21%, Patience=0/40, ETA=1.9h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63     19.1s    0.5171   75.21   % 75.21   % 9.67e-03   🏆 NEW BEST!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:  16%|█▌        | 64/400 [22:10<1:48:01, 19.29s/epoch, Loss=0.5210, Val=75.25%, Best=75.21%, Patience=1/40, ETA=1.8h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64     19.0s    0.5210   75.25   % 75.21   % 9.66e-03   📈 (1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:  16%|█▋        | 65/400 [22:29<1:47:30, 19.25s/epoch, Loss=0.5145, Val=75.82%, Best=75.82%, Patience=0/40, ETA=1.8h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65     19.2s    0.5145   75.82   % 75.82   % 9.64e-03   🏆 NEW BEST!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:  16%|█▋        | 66/400 [22:48<1:46:44, 19.18s/epoch, Loss=0.5115, Val=75.88%, Best=75.82%, Patience=1/40, ETA=1.8h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66     19.0s    0.5115   75.88   % 75.82   % 9.63e-03   📈 (1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:  17%|█▋        | 67/400 [23:07<1:46:16, 19.15s/epoch, Loss=0.5082, Val=75.92%, Best=75.92%, Patience=0/40, ETA=1.8h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67     19.1s    0.5082   75.92   % 75.92   % 9.61e-03   🏆 NEW BEST!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:  17%|█▋        | 68/400 [23:26<1:45:59, 19.15s/epoch, Loss=0.5083, Val=76.26%, Best=76.26%, Patience=0/40, ETA=1.8h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68     19.2s    0.5083   76.26   % 76.26   % 9.60e-03   🏆 NEW BEST!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:  17%|█▋        | 69/400 [23:45<1:45:23, 19.11s/epoch, Loss=0.5068, Val=76.26%, Best=76.26%, Patience=1/40, ETA=1.8h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69     19.0s    0.5068   76.26   % 76.26   % 9.58e-03   📈 (1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:  18%|█▊        | 70/400 [24:04<1:44:53, 19.07s/epoch, Loss=0.5127, Val=76.32%, Best=76.26%, Patience=2/40, ETA=1.7h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70     19.0s    0.5127   76.32   % 76.26   % 9.56e-03   📈 (2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:  18%|█▊        | 71/400 [24:23<1:44:35, 19.07s/epoch, Loss=0.5070, Val=76.39%, Best=76.39%, Patience=0/40, ETA=1.7h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71     19.1s    0.5070   76.39   % 76.39   % 9.55e-03   🏆 NEW BEST!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:  18%|█▊        | 71/400 [24:42<1:44:35, 19.07s/epoch, Loss=0.5065, Val=76.53%, Best=76.53%, Patience=0/40, ETA=1.7h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72     19.1s    0.5065   76.53   % 76.53   % 9.53e-03   🏆 NEW BEST!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:  18%|█▊        | 73/400 [25:01<1:42:48, 18.87s/epoch, Loss=0.5043, Val=76.24%, Best=76.53%, Patience=1/40, ETA=1.7h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73     17.9s    0.5043   76.24   % 76.53   % 9.51e-03   📈 (1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:  18%|█▊        | 74/400 [25:19<1:41:24, 18.66s/epoch, Loss=0.5060, Val=76.70%, Best=76.70%, Patience=0/40, ETA=1.7h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74     18.2s    0.5060   76.70   % 76.70   % 9.49e-03   🏆 NEW BEST!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:  19%|█▉        | 75/400 [25:39<1:42:13, 18.87s/epoch, Loss=0.5017, Val=76.32%, Best=76.70%, Patience=1/40, ETA=1.7h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75     19.4s    0.5017   76.32   % 76.70   % 9.48e-03   📈 (1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:  19%|█▉        | 76/400 [25:58<1:42:42, 19.02s/epoch, Loss=0.5034, Val=76.24%, Best=76.70%, Patience=2/40, ETA=1.7h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76     19.4s    0.5034   76.24   % 76.70   % 9.46e-03   📈 (2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:  19%|█▉        | 77/400 [26:18<1:43:19, 19.19s/epoch, Loss=0.4968, Val=76.34%, Best=76.70%, Patience=3/40, ETA=1.7h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77     19.6s    0.4968   76.34   % 76.70   % 9.44e-03   📈 (3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:  20%|█▉        | 78/400 [26:37<1:43:26, 19.28s/epoch, Loss=0.4971, Val=76.78%, Best=76.78%, Patience=0/40, ETA=1.7h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78     19.5s    0.4971   76.78   % 76.78   % 9.42e-03   🏆 NEW BEST!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:  20%|█▉        | 79/400 [26:57<1:43:42, 19.39s/epoch, Loss=0.4951, Val=76.91%, Best=76.91%, Patience=0/40, ETA=1.7h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79     19.6s    0.4951   76.91   % 76.91   % 9.40e-03   🏆 NEW BEST!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:  20%|██        | 80/400 [27:16<1:43:36, 19.43s/epoch, Loss=0.4965, Val=77.04%, Best=77.04%, Patience=0/40, ETA=1.7h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80     19.5s    0.4965   77.04   % 77.04   % 9.38e-03   🏆 NEW BEST!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:  20%|██        | 81/400 [27:35<1:43:10, 19.41s/epoch, Loss=0.4920, Val=76.97%, Best=77.04%, Patience=1/40, ETA=1.7h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81     19.4s    0.4920   76.97   % 77.04   % 9.36e-03   📈 (1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:  20%|██        | 82/400 [27:55<1:43:12, 19.47s/epoch, Loss=0.4908, Val=77.18%, Best=77.18%, Patience=0/40, ETA=1.7h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82     19.6s    0.4908   77.18   % 77.18   % 9.34e-03   🏆 NEW BEST!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:  21%|██        | 83/400 [28:15<1:42:58, 19.49s/epoch, Loss=0.4903, Val=77.08%, Best=77.18%, Patience=1/40, ETA=1.7h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83     19.5s    0.4903   77.08   % 77.18   % 9.32e-03   📈 (1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:  21%|██        | 83/400 [28:34<1:42:58, 19.49s/epoch, Loss=0.4887, Val=77.14%, Best=77.18%, Patience=2/40, ETA=1.7h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84     19.3s    0.4887   77.14   % 77.18   % 9.30e-03   📈 (2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:  21%|██▏       | 85/400 [28:52<1:39:30, 18.95s/epoch, Loss=0.4861, Val=77.20%, Best=77.18%, Patience=3/40, ETA=1.7h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85     17.3s    0.4861   77.20   % 77.18   % 9.28e-03   📈 (3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:  22%|██▏       | 86/400 [29:11<1:38:53, 18.90s/epoch, Loss=0.4879, Val=76.97%, Best=77.18%, Patience=4/40, ETA=1.7h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86     18.8s    0.4879   76.97   % 77.18   % 9.25e-03   📈 (4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:  22%|██▏       | 87/400 [29:30<1:38:24, 18.86s/epoch, Loss=0.4843, Val=77.06%, Best=77.18%, Patience=5/40, ETA=1.7h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87     18.8s    0.4843   77.06   % 77.18   % 9.23e-03   📈 (5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:  22%|██▏       | 88/400 [29:48<1:38:11, 18.88s/epoch, Loss=0.4810, Val=77.50%, Best=77.50%, Patience=0/40, ETA=1.7h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88     18.9s    0.4810   77.50   % 77.50   % 9.21e-03   🏆 NEW BEST!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:  22%|██▏       | 89/400 [30:07<1:37:49, 18.87s/epoch, Loss=0.4837, Val=77.37%, Best=77.50%, Patience=1/40, ETA=1.6h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89     18.8s    0.4837   77.37   % 77.50   % 9.19e-03   📈 (1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:  22%|██▎       | 90/400 [30:26<1:37:39, 18.90s/epoch, Loss=0.4874, Val=77.98%, Best=77.98%, Patience=0/40, ETA=1.6h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90     19.0s    0.4874   77.98   % 77.98   % 9.17e-03   🏆 NEW BEST!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:  23%|██▎       | 91/400 [30:45<1:37:17, 18.89s/epoch, Loss=0.4792, Val=77.60%, Best=77.98%, Patience=1/40, ETA=1.6h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91     18.9s    0.4792   77.60   % 77.98   % 9.14e-03   📈 (1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:  23%|██▎       | 92/400 [31:04<1:36:46, 18.85s/epoch, Loss=0.4799, Val=77.62%, Best=77.98%, Patience=2/40, ETA=1.6h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92     18.8s    0.4799   77.62   % 77.98   % 9.12e-03   📈 (2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:  23%|██▎       | 93/400 [31:23<1:36:24, 18.84s/epoch, Loss=0.4828, Val=77.60%, Best=77.98%, Patience=3/40, ETA=1.6h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93     18.8s    0.4828   77.60   % 77.98   % 9.10e-03   📈 (3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:  24%|██▎       | 94/400 [31:42<1:36:04, 18.84s/epoch, Loss=0.4794, Val=77.43%, Best=77.98%, Patience=4/40, ETA=1.6h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94     18.8s    0.4794   77.43   % 77.98   % 9.07e-03   📈 (4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:  24%|██▍       | 95/400 [32:00<1:35:40, 18.82s/epoch, Loss=0.4817, Val=77.46%, Best=77.98%, Patience=5/40, ETA=1.6h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95     18.8s    0.4817   77.46   % 77.98   % 9.05e-03   📈 (5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:  24%|██▍       | 95/400 [32:19<1:35:40, 18.82s/epoch, Loss=0.4811, Val=77.20%, Best=77.98%, Patience=6/40, ETA=1.6h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96     18.9s    0.4811   77.20   % 77.98   % 9.02e-03   ⏳ (6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:  24%|██▍       | 97/400 [32:38<1:34:17, 18.67s/epoch, Loss=0.4794, Val=77.35%, Best=77.98%, Patience=7/40, ETA=1.6h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97     17.8s    0.4794   77.35   % 77.98   % 9.00e-03   ⏳ (7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:  24%|██▍       | 98/400 [32:57<1:34:20, 18.74s/epoch, Loss=0.4814, Val=77.35%, Best=77.98%, Patience=8/40, ETA=1.6h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98     18.9s    0.4814   77.35   % 77.98   % 8.97e-03   ⏳ (8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:  25%|██▍       | 99/400 [33:16<1:34:49, 18.90s/epoch, Loss=0.4757, Val=77.41%, Best=77.98%, Patience=9/40, ETA=1.6h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99     19.3s    0.4757   77.41   % 77.98   % 8.95e-03   ⏳ (9)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:  25%|██▌       | 100/400 [33:35<1:34:48, 18.96s/epoch, Loss=0.4825, Val=77.56%, Best=77.98%, Patience=10/40, ETA=1.6h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100    19.1s    0.4825   77.56   % 77.98   % 8.92e-03   ⏳ (10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:  25%|██▌       | 101/400 [33:54<1:34:47, 19.02s/epoch, Loss=0.4777, Val=77.64%, Best=77.98%, Patience=11/40, ETA=1.6h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101    19.2s    0.4777   77.64   % 77.98   % 8.90e-03   ⏳ (11)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:  26%|██▌       | 102/400 [34:13<1:34:36, 19.05s/epoch, Loss=0.4777, Val=77.77%, Best=77.98%, Patience=12/40, ETA=1.6h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102    19.1s    0.4777   77.77   % 77.98   % 8.87e-03   ⏳ (12)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:  26%|██▌       | 103/400 [34:33<1:34:34, 19.10s/epoch, Loss=0.4768, Val=77.67%, Best=77.98%, Patience=13/40, ETA=1.6h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103    19.2s    0.4768   77.67   % 77.98   % 8.85e-03   ⏳ (13)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:  26%|██▌       | 104/400 [34:52<1:34:14, 19.10s/epoch, Loss=0.4757, Val=77.83%, Best=77.98%, Patience=14/40, ETA=1.6h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104    19.1s    0.4757   77.83   % 77.98   % 8.82e-03   ⏳ (14)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:  26%|██▋       | 105/400 [35:11<1:33:53, 19.10s/epoch, Loss=0.4772, Val=77.69%, Best=77.98%, Patience=15/40, ETA=1.6h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105    19.1s    0.4772   77.69   % 77.98   % 8.79e-03   ⏳ (15)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:  26%|██▋       | 106/400 [35:30<1:33:43, 19.13s/epoch, Loss=0.4765, Val=77.52%, Best=77.98%, Patience=16/40, ETA=1.6h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106    19.2s    0.4765   77.52   % 77.98   % 8.77e-03   ⚠️ (16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:  27%|██▋       | 107/400 [35:49<1:33:25, 19.13s/epoch, Loss=0.4734, Val=77.58%, Best=77.98%, Patience=17/40, ETA=1.6h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107    19.1s    0.4734   77.58   % 77.98   % 8.74e-03   ⚠️ (17)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:  27%|██▋       | 107/400 [36:08<1:33:25, 19.13s/epoch, Loss=0.4718, Val=77.54%, Best=77.98%, Patience=18/40, ETA=1.6h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108    19.1s    0.4718   77.54   % 77.98   % 8.71e-03   ⚠️ (18)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:  27%|██▋       | 109/400 [36:26<1:30:41, 18.70s/epoch, Loss=0.4731, Val=77.77%, Best=77.98%, Patience=19/40, ETA=1.5h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109    17.2s    0.4731   77.77   % 77.98   % 8.68e-03   ⚠️ (19)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:  28%|██▊       | 110/400 [36:46<1:32:13, 19.08s/epoch, Loss=0.4717, Val=77.67%, Best=77.98%, Patience=20/40, ETA=1.5h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110    20.0s    0.4717   77.67   % 77.98   % 8.65e-03   ⚠️ (20)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:  28%|██▊       | 111/400 [37:06<1:32:42, 19.25s/epoch, Loss=0.4704, Val=77.67%, Best=77.98%, Patience=21/40, ETA=1.5h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111    19.6s    0.4704   77.67   % 77.98   % 8.63e-03   ⚠️ (21)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:  28%|██▊       | 112/400 [37:25<1:32:49, 19.34s/epoch, Loss=0.4736, Val=77.62%, Best=77.98%, Patience=22/40, ETA=1.5h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112    19.5s    0.4736   77.62   % 77.98   % 8.60e-03   ⚠️ (22)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:  28%|██▊       | 113/400 [37:45<1:32:49, 19.41s/epoch, Loss=0.4708, Val=77.71%, Best=77.98%, Patience=23/40, ETA=1.5h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113    19.6s    0.4708   77.71   % 77.98   % 8.57e-03   ⚠️ (23)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:  28%|██▊       | 114/400 [38:04<1:32:49, 19.48s/epoch, Loss=0.4721, Val=77.94%, Best=77.98%, Patience=24/40, ETA=1.5h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114    19.6s    0.4721   77.94   % 77.98   % 8.54e-03   ⚠️ (24)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:  29%|██▉       | 115/400 [38:24<1:32:50, 19.55s/epoch, Loss=0.4683, Val=77.81%, Best=77.98%, Patience=25/40, ETA=1.5h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115    19.7s    0.4683   77.81   % 77.98   % 8.51e-03   ⚠️ (25)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:  29%|██▉       | 116/400 [38:44<1:32:33, 19.56s/epoch, Loss=0.4704, Val=77.62%, Best=77.98%, Patience=26/40, ETA=1.5h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116    19.6s    0.4704   77.62   % 77.98   % 8.48e-03   ⚠️ (26)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:  29%|██▉       | 117/400 [39:03<1:32:12, 19.55s/epoch, Loss=0.4679, Val=77.90%, Best=77.98%, Patience=27/40, ETA=1.5h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "117    19.5s    0.4679   77.90   % 77.98   % 8.45e-03   ⚠️ (27)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:  30%|██▉       | 118/400 [39:23<1:31:51, 19.54s/epoch, Loss=0.4669, Val=77.75%, Best=77.98%, Patience=28/40, ETA=1.5h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118    19.5s    0.4669   77.75   % 77.98   % 8.42e-03   ⚠️ (28)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:  30%|██▉       | 119/400 [39:42<1:31:34, 19.55s/epoch, Loss=0.4682, Val=77.64%, Best=77.98%, Patience=29/40, ETA=1.5h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119    19.6s    0.4682   77.64   % 77.98   % 8.39e-03   ⚠️ (29)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:  30%|██▉       | 119/400 [40:02<1:31:34, 19.55s/epoch, Loss=0.4663, Val=78.00%, Best=77.98%, Patience=30/40, ETA=1.5h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120    19.6s    0.4663   78.00   % 77.98   % 8.36e-03   ⚠️ (30)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:  30%|███       | 121/400 [40:20<1:28:46, 19.09s/epoch, Loss=0.4698, Val=77.71%, Best=77.98%, Patience=31/40, ETA=1.5h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "121    17.4s    0.4698   77.71   % 77.98   % 8.33e-03   ⚠️ (31)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:  30%|███       | 122/400 [40:41<1:30:58, 19.63s/epoch, Loss=0.4650, Val=77.77%, Best=77.98%, Patience=32/40, ETA=1.5h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122    20.9s    0.4650   77.77   % 77.98   % 8.30e-03   ⚠️ (32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:  31%|███       | 123/400 [41:02<1:32:17, 19.99s/epoch, Loss=0.4650, Val=77.83%, Best=77.98%, Patience=33/40, ETA=1.5h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123    20.8s    0.4650   77.83   % 77.98   % 8.27e-03   ⚠️ (33)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:  31%|███       | 124/400 [41:23<1:33:07, 20.24s/epoch, Loss=0.4656, Val=77.94%, Best=77.98%, Patience=34/40, ETA=1.5h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124    20.8s    0.4656   77.94   % 77.98   % 8.24e-03   ⚠️ (34)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:  31%|███▏      | 125/400 [41:44<1:33:45, 20.46s/epoch, Loss=0.4653, Val=78.06%, Best=78.06%, Patience=0/40, ETA=1.5h] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125    21.0s    0.4653   78.06   % 78.06   % 8.21e-03   🏆 NEW BEST!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:  32%|███▏      | 126/400 [42:05<1:34:04, 20.60s/epoch, Loss=0.4638, Val=77.94%, Best=78.06%, Patience=1/40, ETA=1.5h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126    20.9s    0.4638   77.94   % 78.06   % 8.17e-03   📈 (1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:  32%|███▏      | 127/400 [42:25<1:34:00, 20.66s/epoch, Loss=0.4638, Val=78.09%, Best=78.06%, Patience=2/40, ETA=1.5h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127    20.8s    0.4638   78.09   % 78.06   % 8.14e-03   📈 (2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:  32%|███▏      | 128/400 [42:46<1:34:01, 20.74s/epoch, Loss=0.4656, Val=77.83%, Best=78.06%, Patience=3/40, ETA=1.5h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128    20.9s    0.4656   77.83   % 78.06   % 8.11e-03   📈 (3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:  32%|███▏      | 129/400 [43:07<1:34:00, 20.81s/epoch, Loss=0.4625, Val=77.60%, Best=78.06%, Patience=4/40, ETA=1.5h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129    21.0s    0.4625   77.60   % 78.06   % 8.08e-03   📈 (4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:  32%|███▎      | 130/400 [43:28<1:33:53, 20.86s/epoch, Loss=0.4633, Val=77.60%, Best=78.06%, Patience=5/40, ETA=1.5h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130    21.0s    0.4633   77.60   % 78.06   % 8.04e-03   📈 (5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:  33%|███▎      | 131/400 [43:49<1:33:28, 20.85s/epoch, Loss=0.4633, Val=77.75%, Best=78.06%, Patience=6/40, ETA=1.6h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131    20.8s    0.4633   77.75   % 78.06   % 8.01e-03   ⏳ (6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:  33%|███▎      | 131/400 [44:10<1:33:28, 20.85s/epoch, Loss=0.4643, Val=77.73%, Best=78.06%, Patience=7/40, ETA=1.6h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132    20.8s    0.4643   77.73   % 78.06   % 7.98e-03   ⏳ (7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:  33%|███▎      | 133/400 [44:28<1:28:10, 19.81s/epoch, Loss=0.4624, Val=77.73%, Best=78.06%, Patience=8/40, ETA=1.5h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133    16.9s    0.4624   77.73   % 78.06   % 7.95e-03   ⏳ (8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:  34%|███▎      | 134/400 [44:47<1:27:10, 19.66s/epoch, Loss=0.4615, Val=77.81%, Best=78.06%, Patience=9/40, ETA=1.5h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "134    19.3s    0.4615   77.81   % 78.06   % 7.91e-03   ⏳ (9)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:  34%|███▍      | 135/400 [45:06<1:26:14, 19.53s/epoch, Loss=0.4620, Val=77.83%, Best=78.06%, Patience=10/40, ETA=1.5h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135    19.2s    0.4620   77.83   % 78.06   % 7.88e-03   ⏳ (10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:  34%|███▍      | 136/400 [45:25<1:25:25, 19.42s/epoch, Loss=0.4630, Val=77.96%, Best=78.06%, Patience=11/40, ETA=1.5h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136    19.2s    0.4630   77.96   % 78.06   % 7.84e-03   ⏳ (11)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:  34%|███▍      | 137/400 [45:45<1:25:10, 19.43s/epoch, Loss=0.4599, Val=78.04%, Best=78.06%, Patience=12/40, ETA=1.5h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137    19.5s    0.4599   78.04   % 78.06   % 7.81e-03   ⏳ (12)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:  34%|███▍      | 138/400 [46:04<1:25:02, 19.47s/epoch, Loss=0.4614, Val=77.96%, Best=78.06%, Patience=13/40, ETA=1.4h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138    19.6s    0.4614   77.96   % 78.06   % 7.78e-03   ⏳ (13)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:  35%|███▍      | 139/400 [46:24<1:24:28, 19.42s/epoch, Loss=0.4632, Val=77.94%, Best=78.06%, Patience=14/40, ETA=1.4h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "139    19.3s    0.4632   77.94   % 78.06   % 7.74e-03   ⏳ (14)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:  35%|███▌      | 140/400 [46:43<1:23:58, 19.38s/epoch, Loss=0.4606, Val=77.94%, Best=78.06%, Patience=15/40, ETA=1.4h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140    19.3s    0.4606   77.94   % 78.06   % 7.71e-03   ⏳ (15)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:  35%|███▌      | 141/400 [47:02<1:23:41, 19.39s/epoch, Loss=0.4595, Val=77.92%, Best=78.06%, Patience=16/40, ETA=1.4h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141    19.4s    0.4595   77.92   % 78.06   % 7.67e-03   ⚠️ (16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:  36%|███▌      | 142/400 [47:22<1:23:29, 19.42s/epoch, Loss=0.4607, Val=77.88%, Best=78.06%, Patience=17/40, ETA=1.4h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "142    19.5s    0.4607   77.88   % 78.06   % 7.64e-03   ⚠️ (17)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:  36%|███▌      | 143/400 [47:41<1:22:53, 19.35s/epoch, Loss=0.4608, Val=77.81%, Best=78.06%, Patience=18/40, ETA=1.4h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143    19.2s    0.4608   77.81   % 78.06   % 7.60e-03   ⚠️ (18)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:  36%|███▌      | 143/400 [48:00<1:22:53, 19.35s/epoch, Loss=0.4604, Val=78.17%, Best=78.17%, Patience=0/40, ETA=1.4h] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144    19.4s    0.4604   78.17   % 78.17   % 7.57e-03   🏆 NEW BEST!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:  36%|███▋      | 145/400 [48:18<1:20:09, 18.86s/epoch, Loss=0.4578, Val=78.27%, Best=78.27%, Patience=0/40, ETA=1.4h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "145    17.2s    0.4578   78.27   % 78.27   % 7.53e-03   🏆 NEW BEST!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:  36%|███▋      | 146/400 [48:38<1:21:13, 19.19s/epoch, Loss=0.4588, Val=78.04%, Best=78.27%, Patience=1/40, ETA=1.4h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "146    19.9s    0.4588   78.04   % 78.27   % 7.50e-03   📈 (1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:  37%|███▋      | 147/400 [48:58<1:21:59, 19.44s/epoch, Loss=0.4588, Val=78.21%, Best=78.27%, Patience=2/40, ETA=1.4h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "147    20.0s    0.4588   78.21   % 78.27   % 7.46e-03   📈 (2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:  37%|███▋      | 148/400 [49:18<1:22:22, 19.61s/epoch, Loss=0.4605, Val=78.00%, Best=78.27%, Patience=3/40, ETA=1.4h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "148    20.0s    0.4605   78.00   % 78.27   % 7.42e-03   📈 (3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:  37%|███▋      | 149/400 [49:38<1:22:32, 19.73s/epoch, Loss=0.4616, Val=77.96%, Best=78.27%, Patience=4/40, ETA=1.4h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "149    20.0s    0.4616   77.96   % 78.27   % 7.39e-03   📈 (4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:  38%|███▊      | 150/400 [49:58<1:22:46, 19.87s/epoch, Loss=0.4599, Val=78.04%, Best=78.27%, Patience=5/40, ETA=1.4h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150    20.2s    0.4599   78.04   % 78.27   % 7.35e-03   📈 (5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:  38%|███▊      | 151/400 [50:19<1:22:49, 19.96s/epoch, Loss=0.4581, Val=77.94%, Best=78.27%, Patience=6/40, ETA=1.4h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151    20.2s    0.4581   77.94   % 78.27   % 7.32e-03   ⏳ (6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:  38%|███▊      | 152/400 [50:39<1:22:50, 20.04s/epoch, Loss=0.4582, Val=77.83%, Best=78.27%, Patience=7/40, ETA=1.4h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152    20.2s    0.4582   77.83   % 78.27   % 7.28e-03   ⏳ (7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:  38%|███▊      | 153/400 [50:59<1:22:47, 20.11s/epoch, Loss=0.4578, Val=77.98%, Best=78.27%, Patience=8/40, ETA=1.4h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153    20.3s    0.4578   77.98   % 78.27   % 7.24e-03   ⏳ (8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:  38%|███▊      | 154/400 [51:19<1:22:28, 20.12s/epoch, Loss=0.4586, Val=77.94%, Best=78.27%, Patience=9/40, ETA=1.4h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "154    20.1s    0.4586   77.94   % 78.27   % 7.20e-03   ⏳ (9)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:  39%|███▉      | 155/400 [51:39<1:22:07, 20.11s/epoch, Loss=0.4574, Val=78.00%, Best=78.27%, Patience=10/40, ETA=1.4h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "155    20.1s    0.4574   78.00   % 78.27   % 7.17e-03   ⏳ (10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:  39%|███▉      | 155/400 [52:00<1:22:07, 20.11s/epoch, Loss=0.4585, Val=77.96%, Best=78.27%, Patience=11/40, ETA=1.4h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "156    20.1s    0.4585   77.96   % 78.27   % 7.13e-03   ⏳ (11)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:  39%|███▉      | 157/400 [52:17<1:18:23, 19.35s/epoch, Loss=0.4580, Val=78.15%, Best=78.27%, Patience=12/40, ETA=1.3h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157    16.8s    0.4580   78.15   % 78.27   % 7.09e-03   ⏳ (12)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:  40%|███▉      | 158/400 [52:37<1:17:47, 19.29s/epoch, Loss=0.4573, Val=77.98%, Best=78.27%, Patience=13/40, ETA=1.3h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "158    19.1s    0.4573   77.98   % 78.27   % 7.06e-03   ⏳ (13)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:  40%|███▉      | 159/400 [52:56<1:17:12, 19.22s/epoch, Loss=0.4583, Val=78.25%, Best=78.27%, Patience=14/40, ETA=1.3h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159    19.1s    0.4583   78.25   % 78.27   % 7.02e-03   ⏳ (14)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:  40%|████      | 160/400 [53:14<1:16:15, 19.06s/epoch, Loss=0.4572, Val=78.13%, Best=78.27%, Patience=15/40, ETA=1.3h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160    18.7s    0.4572   78.13   % 78.27   % 6.98e-03   ⏳ (15)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:  40%|████      | 161/400 [53:33<1:15:34, 18.97s/epoch, Loss=0.4568, Val=78.09%, Best=78.27%, Patience=16/40, ETA=1.3h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "161    18.8s    0.4568   78.09   % 78.27   % 6.94e-03   ⚠️ (16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:  40%|████      | 162/400 [53:52<1:14:56, 18.89s/epoch, Loss=0.4566, Val=78.23%, Best=78.27%, Patience=17/40, ETA=1.3h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162    18.7s    0.4566   78.23   % 78.27   % 6.90e-03   ⚠️ (17)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:  41%|████      | 163/400 [54:11<1:14:45, 18.93s/epoch, Loss=0.4557, Val=78.21%, Best=78.27%, Patience=18/40, ETA=1.3h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "163    19.0s    0.4557   78.21   % 78.27   % 6.87e-03   ⚠️ (18)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:  41%|████      | 164/400 [54:30<1:14:25, 18.92s/epoch, Loss=0.4575, Val=78.21%, Best=78.27%, Patience=19/40, ETA=1.2h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "164    18.9s    0.4575   78.21   % 78.27   % 6.83e-03   ⚠️ (19)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:  41%|████▏     | 165/400 [54:49<1:14:09, 18.93s/epoch, Loss=0.4551, Val=78.21%, Best=78.27%, Patience=20/40, ETA=1.2h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "165    19.0s    0.4551   78.21   % 78.27   % 6.79e-03   ⚠️ (20)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:  42%|████▏     | 166/400 [55:08<1:13:43, 18.90s/epoch, Loss=0.4567, Val=78.15%, Best=78.27%, Patience=21/40, ETA=1.2h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "166    18.8s    0.4567   78.15   % 78.27   % 6.75e-03   ⚠️ (21)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:  42%|████▏     | 167/400 [55:26<1:13:26, 18.91s/epoch, Loss=0.4528, Val=78.27%, Best=78.27%, Patience=22/40, ETA=1.2h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "167    18.9s    0.4528   78.27   % 78.27   % 6.71e-03   ⚠️ (22)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:  42%|████▏     | 167/400 [55:45<1:13:26, 18.91s/epoch, Loss=0.4552, Val=78.17%, Best=78.27%, Patience=23/40, ETA=1.2h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "168    18.8s    0.4552   78.17   % 78.27   % 6.67e-03   ⚠️ (23)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:  42%|████▏     | 169/400 [56:03<1:11:25, 18.55s/epoch, Loss=0.4554, Val=77.96%, Best=78.27%, Patience=24/40, ETA=1.2h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169    17.0s    0.4554   77.96   % 78.27   % 6.63e-03   ⚠️ (24)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:  42%|████▎     | 170/400 [56:22<1:11:28, 18.65s/epoch, Loss=0.4558, Val=78.02%, Best=78.27%, Patience=25/40, ETA=1.2h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "170    18.9s    0.4558   78.02   % 78.27   % 6.60e-03   ⚠️ (25)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:  43%|████▎     | 171/400 [56:42<1:12:27, 18.98s/epoch, Loss=0.4560, Val=77.81%, Best=78.27%, Patience=26/40, ETA=1.2h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "171    19.8s    0.4560   77.81   % 78.27   % 6.56e-03   ⚠️ (26)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:  43%|████▎     | 172/400 [57:02<1:12:45, 19.15s/epoch, Loss=0.4554, Val=77.69%, Best=78.27%, Patience=27/40, ETA=1.2h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "172    19.5s    0.4554   77.69   % 78.27   % 6.52e-03   ⚠️ (27)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:  43%|████▎     | 173/400 [57:21<1:12:47, 19.24s/epoch, Loss=0.4537, Val=77.73%, Best=78.27%, Patience=28/40, ETA=1.2h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "173    19.4s    0.4537   77.73   % 78.27   % 6.48e-03   ⚠️ (28)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:  44%|████▎     | 174/400 [57:40<1:12:41, 19.30s/epoch, Loss=0.4539, Val=77.75%, Best=78.27%, Patience=29/40, ETA=1.2h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "174    19.4s    0.4539   77.75   % 78.27   % 6.44e-03   ⚠️ (29)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:  44%|████▍     | 175/400 [58:00<1:12:36, 19.36s/epoch, Loss=0.4550, Val=77.90%, Best=78.27%, Patience=30/40, ETA=1.2h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175    19.5s    0.4550   77.90   % 78.27   % 6.40e-03   ⚠️ (30)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:  44%|████▍     | 176/400 [58:19<1:12:15, 19.35s/epoch, Loss=0.4566, Val=77.79%, Best=78.27%, Patience=31/40, ETA=1.2h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "176    19.3s    0.4566   77.79   % 78.27   % 6.36e-03   ⚠️ (31)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:  44%|████▍     | 177/400 [58:39<1:12:03, 19.39s/epoch, Loss=0.4552, Val=77.96%, Best=78.27%, Patience=32/40, ETA=1.2h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "177    19.5s    0.4552   77.96   % 78.27   % 6.32e-03   ⚠️ (32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:  44%|████▍     | 178/400 [58:58<1:11:41, 19.38s/epoch, Loss=0.4538, Val=77.98%, Best=78.27%, Patience=33/40, ETA=1.2h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "178    19.3s    0.4538   77.98   % 78.27   % 6.28e-03   ⚠️ (33)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:  45%|████▍     | 179/400 [59:17<1:11:23, 19.38s/epoch, Loss=0.4567, Val=77.88%, Best=78.27%, Patience=34/40, ETA=1.2h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179    19.4s    0.4567   77.88   % 78.27   % 6.24e-03   ⚠️ (34)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:  45%|████▍     | 179/400 [59:37<1:11:23, 19.38s/epoch, Loss=0.4547, Val=78.00%, Best=78.27%, Patience=35/40, ETA=1.2h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180    19.6s    0.4547   78.00   % 78.27   % 6.20e-03   ⚠️ (35)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:  45%|████▌     | 181/400 [59:55<1:09:35, 19.07s/epoch, Loss=0.4545, Val=78.00%, Best=78.27%, Patience=36/40, ETA=1.2h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "181    17.6s    0.4545   78.00   % 78.27   % 6.16e-03   ⚠️ (36)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:  46%|████▌     | 182/400 [1:00:14<1:08:39, 18.89s/epoch, Loss=0.4558, Val=77.94%, Best=78.27%, Patience=37/40, ETA=1.2h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "182    18.5s    0.4558   77.94   % 78.27   % 6.12e-03   ⚠️ (37)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:  46%|████▌     | 183/400 [1:00:32<1:07:31, 18.67s/epoch, Loss=0.4553, Val=77.83%, Best=78.27%, Patience=38/40, ETA=1.1h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "183    18.1s    0.4553   77.83   % 78.27   % 6.08e-03   ⚠️ (38)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:  46%|████▌     | 184/400 [1:00:50<1:06:40, 18.52s/epoch, Loss=0.4533, Val=77.88%, Best=78.27%, Patience=39/40, ETA=1.1h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184    18.2s    0.4533   77.88   % 78.27   % 6.04e-03   ⚠️ (39)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Optimized Training:  46%|████▌     | 184/400 [1:01:08<1:11:46, 19.94s/epoch, Loss=0.4527, Val=77.92%, Best=78.27%, Patience=40/40, ETA=1.1h]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "185    18.1s    0.4527   77.92   % 78.27   % 6.00e-03   ⚠️ (40)\n",
      "\n",
      "⏹️ Early stopping triggered at epoch 185\n",
      "\n",
      "================================================================================\n",
      "🏁 OPTIMIZED TRAINING COMPLETE!\n",
      "================================================================================\n",
      "📊 TRAINING SUMMARY:\n",
      "   Total training time: 1.0h\n",
      "   Average time per epoch: 19.8s\n",
      "   Best epoch: 145\n",
      "   Best accuracy: 0.7827 (78.27%)\n",
      "   Total epochs trained: 185\n",
      "✅ Loaded best model from epoch 145\n",
      "\n",
      "📈 IMPROVEMENT ANALYSIS:\n",
      "   Starting baseline: 77.8%\n",
      "   OPTIMIZED result: 78.27%\n",
      "   Net improvement: +0.47 percentage points\n",
      "   Model parameters: 10,380,433 (10.38M)\n",
      "   Parameter efficiency: 0.05 pp/M params\n",
      "\n",
      "💎 OPTIMIZED achievement: 78.27% with 10.4M parameters!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "💎🎯 FINAL OPTIMIZED STATUS:\n",
      "💎 Optimized progress: 78.27%\n"
     ]
    }
   ],
   "source": [
    "# ===== RESTART KERNEL FIRST AND RUN THIS COMPLETE CODE =====\n",
    "# OPTIMIZED 5-8M PARAMETER MODEL WITH PROGRESS BAR AND EVERY EPOCH RESULTS\n",
    "\n",
    "# Restart your kernel first, then run this\n",
    "import sys\n",
    "import os\n",
    "import gc\n",
    "import math\n",
    "import warnings\n",
    "import time\n",
    "from datetime import datetime\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Safe torch imports\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.nn.functional as F\n",
    "    print(\"✅ PyTorch loaded successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"❌ PyTorch import failed: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Progress bar import\n",
    "try:\n",
    "    from tqdm import tqdm\n",
    "    print(\"✅ tqdm loaded for progress bars\")\n",
    "except ImportError:\n",
    "    print(\"⚠️ tqdm not found, using basic progress\")\n",
    "    class tqdm:\n",
    "        def __init__(self, iterable=None, total=None, desc=\"\", unit=\"epoch\"):\n",
    "            self.iterable = iterable if iterable else range(total)\n",
    "            self.total = total\n",
    "            self.desc = desc\n",
    "            self.current = 0\n",
    "            \n",
    "        def __iter__(self):\n",
    "            return self\n",
    "            \n",
    "        def __next__(self):\n",
    "            if self.current >= len(self.iterable):\n",
    "                raise StopIteration\n",
    "            value = self.iterable[self.current]\n",
    "            self.current += 1\n",
    "            progress = self.current / len(self.iterable) * 100\n",
    "            print(f\"\\r{self.desc} [{self.current}/{len(self.iterable)}] {progress:.1f}%\", end='', flush=True)\n",
    "            return value\n",
    "            \n",
    "        def set_postfix(self, **kwargs):\n",
    "            pass\n",
    "\n",
    "# Safe PyTorch Geometric imports with fallback\n",
    "try:\n",
    "    import torch_geometric\n",
    "    from torch_geometric.nn import HeteroConv, GATConv, SAGEConv\n",
    "    from torch_geometric.nn import Linear\n",
    "    print(\"✅ PyTorch Geometric loaded successfully\")\n",
    "except (ImportError, AttributeError) as e:\n",
    "    print(f\"⚠️ PyTorch Geometric issue: {e}\")\n",
    "    print(\"🔧 Using fallback Linear layer...\")\n",
    "    Linear = nn.Linear\n",
    "\n",
    "# Check CUDA availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"✅ Using device: {device}\")\n",
    "\n",
    "class OptimizedGNN58M(nn.Module):\n",
    "    \"\"\"Optimized 5-8M Parameter GNN for Maximum Accuracy\"\"\"\n",
    "    def __init__(self, sequence_features, problem_features, skill_features,\n",
    "                 hidden_dim=320, num_layers=5, num_heads=20, dropout=0.16, device='cuda'):\n",
    "        super(OptimizedGNN58M, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        self.num_heads = num_heads\n",
    "        self.device = device\n",
    "        \n",
    "        # OPTIMIZED input projections - 3x expansion (reduced from 4x)\n",
    "        self.sequence_input_proj = nn.Sequential(\n",
    "            nn.Linear(sequence_features, hidden_dim * 3),  # 3x instead of 4x\n",
    "            nn.BatchNorm1d(hidden_dim * 3),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim * 3, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.GELU()\n",
    "        ).to(device)\n",
    "        \n",
    "        self.problem_input_proj = nn.Sequential(\n",
    "            nn.Linear(problem_features, hidden_dim * 3),\n",
    "            nn.BatchNorm1d(hidden_dim * 3),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim * 3, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.GELU()\n",
    "        ).to(device)\n",
    "        \n",
    "        self.skill_input_proj = nn.Sequential(\n",
    "            nn.Linear(skill_features, hidden_dim * 3),\n",
    "            nn.BatchNorm1d(hidden_dim * 3),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim * 3, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.GELU()\n",
    "        ).to(device)\n",
    "        \n",
    "        # Optimized heterogeneous layers - 5 layers (reduced from 6)\n",
    "        self.conv_layers = nn.ModuleList()\n",
    "        self.batch_norms = nn.ModuleList()\n",
    "        self.residual_projections = nn.ModuleList()\n",
    "        \n",
    "        for i in range(num_layers):\n",
    "            try:\n",
    "                conv_dict = {\n",
    "                    ('sequence', 'predicts', 'problem'): GATConv(\n",
    "                        hidden_dim, hidden_dim // num_heads, heads=num_heads,\n",
    "                        dropout=dropout, add_self_loops=False, edge_dim=9, concat=True\n",
    "                    ),\n",
    "                    ('problem', 'predicted_by', 'sequence'): GATConv(\n",
    "                        hidden_dim, hidden_dim // num_heads, heads=num_heads,\n",
    "                        dropout=dropout, add_self_loops=False, concat=True\n",
    "                    ),\n",
    "                    ('problem', 'requires', 'skill'): SAGEConv(\n",
    "                        (hidden_dim, hidden_dim), hidden_dim, aggr='mean'\n",
    "                    ),\n",
    "                    ('skill', 'required_by', 'problem'): SAGEConv(\n",
    "                        (hidden_dim, hidden_dim), hidden_dim, aggr='mean'\n",
    "                    ),\n",
    "                }\n",
    "                self.conv_layers.append(HeteroConv(conv_dict, aggr='mean').to(device))\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Graph layer {i} failed: {e}\")\n",
    "                fallback_layer = nn.Linear(hidden_dim, hidden_dim).to(device)\n",
    "                self.conv_layers.append(fallback_layer)\n",
    "            \n",
    "            bn_dict = nn.ModuleDict({\n",
    "                'sequence': nn.Sequential(nn.BatchNorm1d(hidden_dim), nn.Dropout(dropout)).to(device),\n",
    "                'problem': nn.Sequential(nn.BatchNorm1d(hidden_dim), nn.Dropout(dropout)).to(device),\n",
    "                'skill': nn.Sequential(nn.BatchNorm1d(hidden_dim), nn.Dropout(dropout)).to(device)\n",
    "            })\n",
    "            self.batch_norms.append(bn_dict)\n",
    "            \n",
    "            residual_dict = nn.ModuleDict({\n",
    "                'sequence': nn.Linear(hidden_dim, hidden_dim, bias=False) if i > 0 else nn.Identity(),\n",
    "                'problem': nn.Linear(hidden_dim, hidden_dim, bias=False) if i > 0 else nn.Identity(),\n",
    "                'skill': nn.Linear(hidden_dim, hidden_dim, bias=False) if i > 0 else nn.Identity()\n",
    "            })\n",
    "            self.residual_projections.append(residual_dict.to(device))\n",
    "        \n",
    "        # OPTIMIZED temporal encoder - 4x expansion (reduced from 5x)\n",
    "        self.temporal_encoder = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim * 4),\n",
    "            nn.BatchNorm1d(hidden_dim * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim * 4, hidden_dim * 2),\n",
    "            nn.BatchNorm1d(hidden_dim * 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout * 0.8),\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.GELU()\n",
    "        ).to(device)\n",
    "        \n",
    "        # OPTIMIZED predictor - Reduced depth but maintained capacity\n",
    "        self.predictor = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim * 4),  # 4x expansion\n",
    "            nn.BatchNorm1d(hidden_dim * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout * 2.0),\n",
    "            \n",
    "            nn.Linear(hidden_dim * 4, hidden_dim * 3),  # 3x layer\n",
    "            nn.BatchNorm1d(hidden_dim * 3),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout * 1.6),\n",
    "            \n",
    "            nn.Linear(hidden_dim * 3, hidden_dim * 2),  # 2x layer\n",
    "            nn.BatchNorm1d(hidden_dim * 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout * 1.2),\n",
    "            \n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),  # 1x layer\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            \n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),  # 0.5x layer\n",
    "            nn.BatchNorm1d(hidden_dim // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout * 0.6),\n",
    "            \n",
    "            nn.Linear(hidden_dim // 2, 1)  # Output\n",
    "        ).to(device)\n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.xavier_normal_(module.weight, gain=1.05)  # Slightly reduced gain\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.BatchNorm1d):\n",
    "            nn.init.ones_(module.weight)\n",
    "            nn.init.zeros_(module.bias)\n",
    "    \n",
    "    def safe_hetero_conv(self, conv_layer, x_dict, edge_index_dict, edge_attr_dict=None):\n",
    "        try:\n",
    "            if hasattr(conv_layer, 'convs'):\n",
    "                if edge_attr_dict:\n",
    "                    return conv_layer(x_dict, edge_index_dict, edge_attr_dict)\n",
    "                else:\n",
    "                    return conv_layer(x_dict, edge_index_dict)\n",
    "            else:\n",
    "                return {k: conv_layer(v) for k, v in x_dict.items()}\n",
    "        except Exception as e:\n",
    "            return x_dict\n",
    "    \n",
    "    def forward(self, data):\n",
    "        try:\n",
    "            x_dict = {\n",
    "                'sequence': self.sequence_input_proj(data['sequence'].x.to(self.device)),\n",
    "                'problem': self.problem_input_proj(data['problem'].x.to(self.device)),\n",
    "                'skill': self.skill_input_proj(data['skill'].x.to(self.device))\n",
    "            }\n",
    "            \n",
    "            for i, (conv, bn_dict, res_dict) in enumerate(zip(self.conv_layers, self.batch_norms, self.residual_projections)):\n",
    "                residual = {k: res_dict[k](v) for k, v in x_dict.items()} if i > 0 else None\n",
    "                \n",
    "                if i == 0 and hasattr(data, 'edge_index_dict') and ('sequence', 'predicts', 'problem') in data.edge_index_dict:\n",
    "                    edge_attr_dict = {\n",
    "                        ('sequence', 'predicts', 'problem'): data['sequence', 'predicts', 'problem'].edge_attr.to(self.device)\n",
    "                    }\n",
    "                    x_dict = self.safe_hetero_conv(conv, x_dict, data.edge_index_dict, edge_attr_dict)\n",
    "                else:\n",
    "                    x_dict = self.safe_hetero_conv(conv, x_dict, getattr(data, 'edge_index_dict', {}))\n",
    "                \n",
    "                for node_type in x_dict.keys():\n",
    "                    if node_type in bn_dict:\n",
    "                        x_dict[node_type] = bn_dict[node_type](x_dict[node_type])\n",
    "                        x_dict[node_type] = F.gelu(x_dict[node_type])\n",
    "                        \n",
    "                        if residual is not None and node_type in residual:\n",
    "                            x_dict[node_type] = x_dict[node_type] + 0.15 * residual[node_type]\n",
    "            \n",
    "            sequence_embeddings = self.temporal_encoder(x_dict['sequence'])\n",
    "            problem_embeddings = x_dict['problem']\n",
    "            \n",
    "            if hasattr(data, 'edge_index_dict') and ('sequence', 'predicts', 'problem') in data.edge_index_dict:\n",
    "                edge_index = data['sequence', 'predicts', 'problem'].edge_index.to(self.device)\n",
    "            else:\n",
    "                seq_size = sequence_embeddings.shape[0]\n",
    "                prob_size = problem_embeddings.shape[0]\n",
    "                edge_index = torch.stack([\n",
    "                    torch.arange(min(seq_size, prob_size), device=self.device),\n",
    "                    torch.arange(min(seq_size, prob_size), device=self.device)\n",
    "                ])\n",
    "            \n",
    "            sequence_indices = edge_index[0]\n",
    "            problem_indices = edge_index[1]\n",
    "            \n",
    "            pred_sequence_emb = sequence_embeddings[sequence_indices]\n",
    "            pred_problem_emb = problem_embeddings[problem_indices]\n",
    "            combined_embeddings = torch.cat([pred_sequence_emb, pred_problem_emb], dim=1)\n",
    "            \n",
    "            logits = self.predictor(combined_embeddings).squeeze(-1)\n",
    "            \n",
    "            return logits, {\n",
    "                'sequence_embeddings': sequence_embeddings,\n",
    "                'problem_embeddings': problem_embeddings\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            batch_size = data['sequence'].x.shape[0]\n",
    "            return torch.zeros(batch_size, device=self.device), {}\n",
    "\n",
    "class OptimizedSymbolicEngine58M(nn.Module):\n",
    "    \"\"\"Optimized 5-8M Parameter Symbolic Engine\"\"\"\n",
    "    def __init__(self, device='cuda'):\n",
    "        super(OptimizedSymbolicEngine58M, self).__init__()\n",
    "        self.device = device\n",
    "        \n",
    "        # Enhanced but not excessive rule weights\n",
    "        self.rule_weights = nn.Parameter(torch.tensor([\n",
    "            12.0, 10.5, 9.5, 8.5, 7.5, 6.5, 5.5, 4.5\n",
    "        ], device=device))\n",
    "        \n",
    "        # Optimized thresholds\n",
    "        self.performance_threshold = nn.Parameter(torch.tensor(0.82, device=device))\n",
    "        self.hint_threshold = nn.Parameter(torch.tensor(0.18, device=device))\n",
    "        self.skill_threshold = nn.Parameter(torch.tensor(0.78, device=device))\n",
    "        self.time_optimal = nn.Parameter(torch.tensor(0.62, device=device))\n",
    "        \n",
    "        # OPTIMIZED rule combiner - Reduced size but still powerful\n",
    "        self.rule_combiner = nn.Sequential(\n",
    "            nn.Linear(9, 1024),  # 8 rules + 1 GNN - Reduced from 1536\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.4),\n",
    "            \n",
    "            nn.Linear(1024, 512),  # Reduced from 768\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.35),\n",
    "            \n",
    "            nn.Linear(512, 256),  # Reduced from 384\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            nn.Linear(128, 64),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(64, 1)\n",
    "        ).to(device)\n",
    "        \n",
    "    def safe_extract_features(self, edge_features, gnn_probs):\n",
    "        batch_size = gnn_probs.shape[0]\n",
    "        \n",
    "        try:\n",
    "            hint_usage = torch.clamp(\n",
    "                edge_features[:, 6] if edge_features.shape[1] > 6 \n",
    "                else (1 - gnn_probs) * 0.15, 0, 1\n",
    "            ).to(self.device)\n",
    "            \n",
    "            time_ratio = torch.clamp(\n",
    "                edge_features[:, 5] if edge_features.shape[1] > 5 \n",
    "                else 0.5 + (gnn_probs - 0.5) * 0.15, 0, 2\n",
    "            ).to(self.device)\n",
    "            \n",
    "            skill_overlap = torch.clamp(\n",
    "                edge_features[:, 2] if edge_features.shape[1] > 2 \n",
    "                else gnn_probs * 0.25 + 0.6, 0, 1\n",
    "            ).to(self.device)\n",
    "            \n",
    "            performance_history = torch.clamp(\n",
    "                edge_features[:, 8] if edge_features.shape[1] > 8 \n",
    "                else gnn_probs * 0.98 + 0.01, 0, 1\n",
    "            ).to(self.device)\n",
    "            \n",
    "            recent_trend = torch.clamp(\n",
    "                edge_features[:, 7] if edge_features.shape[1] > 7 \n",
    "                else (gnn_probs - 0.5) * 1.5, -1, 1\n",
    "            ).to(self.device)\n",
    "            \n",
    "        except Exception as e:\n",
    "            hint_usage = (1 - gnn_probs) * 0.25\n",
    "            time_ratio = torch.ones_like(gnn_probs) * 0.5\n",
    "            skill_overlap = gnn_probs * 0.3 + 0.5\n",
    "            performance_history = gnn_probs * 0.85 + 0.1\n",
    "            recent_trend = (gnn_probs - 0.5) * 1.0\n",
    "        \n",
    "        return hint_usage, time_ratio, skill_overlap, performance_history, recent_trend\n",
    "    \n",
    "    def apply_optimized_rules(self, gnn_logits, edge_features):\n",
    "        \"\"\"Apply 8 OPTIMIZED high-impact rules\"\"\"\n",
    "        batch_size = gnn_logits.shape[0]\n",
    "        gnn_probs = torch.sigmoid(gnn_logits)\n",
    "        \n",
    "        hint_usage, time_ratio, skill_overlap, performance_history, recent_trend = self.safe_extract_features(edge_features, gnn_probs)\n",
    "        \n",
    "        confidence_level = torch.abs(gnn_probs - 0.5) * 2\n",
    "        difficulty_level = 1.0 - performance_history\n",
    "        mastery_level = skill_overlap * performance_history\n",
    "        \n",
    "        rules = torch.zeros(batch_size, 8, device=self.device)\n",
    "        \n",
    "        try:\n",
    "            # Rule 1: Performance Consistency - 12x boost\n",
    "            consistency = torch.sigmoid((performance_history - self.performance_threshold) * 45)\n",
    "            confidence_amp = 1 + confidence_level * 1.2\n",
    "            rules[:, 0] = 12.0 * consistency * confidence_amp\n",
    "            \n",
    "            # Rule 2: Skill Mastery Transfer - 10.5x boost\n",
    "            transfer = skill_overlap * performance_history\n",
    "            transfer_boost = torch.sigmoid((transfer - self.skill_threshold) * 40)\n",
    "            neural_synergy = 1 + 0.6 * gnn_probs\n",
    "            rules[:, 1] = 10.5 * transfer_boost * neural_synergy\n",
    "            \n",
    "            # Rule 3: Learning Momentum - 9.5x boost\n",
    "            momentum = torch.sigmoid(recent_trend * 30)\n",
    "            momentum_amp = 1 + 0.5 * performance_history\n",
    "            rules[:, 2] = 9.5 * momentum * momentum_amp\n",
    "            \n",
    "            # Rule 4: Strategic Hint Usage - 8.5x boost\n",
    "            strategic_hints = hint_usage * difficulty_level\n",
    "            appropriate = torch.sigmoid(-(hint_usage - self.hint_threshold) * 60)\n",
    "            rules[:, 3] = 8.5 * strategic_hints * appropriate\n",
    "            \n",
    "            # Rule 5: Time Appropriateness - 7.5x boost\n",
    "            time_optimality = torch.exp(-25 * (time_ratio - self.time_optimal) ** 2)\n",
    "            time_confidence = 1 + 0.4 * confidence_level\n",
    "            rules[:, 4] = 7.5 * time_optimality * time_confidence\n",
    "            \n",
    "            # Rule 6: Difficulty Alignment - 6.5x boost\n",
    "            zpd_alignment = torch.exp(-35 * (difficulty_level - (1 - performance_history)) ** 2)\n",
    "            rules[:, 5] = 6.5 * zpd_alignment\n",
    "            \n",
    "            # Rule 7: Practice Quality - 5.5x boost\n",
    "            practice_quality = torch.clamp(recent_trend, 0, 1) * (1 - hint_usage)\n",
    "            rules[:, 6] = 5.5 * practice_quality\n",
    "            \n",
    "            # Rule 8: Content Familiarity - 4.5x boost\n",
    "            familiarity = skill_overlap * performance_history * (1 + 0.2 * confidence_level)\n",
    "            rules[:, 7] = 4.5 * familiarity\n",
    "            \n",
    "        except Exception as e:\n",
    "            for i in range(8):\n",
    "                rules[:, i] = self.rule_weights[i] * gnn_probs * (i + 1) / 8\n",
    "        \n",
    "        return rules\n",
    "    \n",
    "    def forward(self, gnn_logits, graph_data):\n",
    "        try:\n",
    "            if hasattr(graph_data, 'edge_attr'):\n",
    "                edge_features = graph_data.edge_attr.to(self.device)\n",
    "            elif hasattr(graph_data, '__getitem__') and ('sequence', 'predicts', 'problem') in graph_data:\n",
    "                edge_features = graph_data['sequence', 'predicts', 'problem'].edge_attr.to(self.device)\n",
    "            else:\n",
    "                batch_size = gnn_logits.shape[0]\n",
    "                edge_features = torch.randn(batch_size, 10, device=self.device) * 0.1\n",
    "            \n",
    "            rule_outputs = self.apply_optimized_rules(gnn_logits, edge_features)\n",
    "            weighted_rules = rule_outputs * self.rule_weights.unsqueeze(0)\n",
    "            \n",
    "            gnn_probs = torch.sigmoid(gnn_logits)\n",
    "            combined_input = torch.cat([gnn_probs.unsqueeze(1), weighted_rules], dim=1)\n",
    "            \n",
    "            adjustment = self.rule_combiner(combined_input).squeeze(1)\n",
    "            \n",
    "            # OPTIMIZED scaling - 12x to 25x (reduced from 20x to 50x)\n",
    "            uncertainty = 1 - torch.abs(gnn_probs - 0.5) * 2\n",
    "            confidence_scaling = torch.abs(gnn_probs - 0.5) * 2\n",
    "            adaptive_scale = 12.0 + 10.0 * uncertainty + 6.0 * confidence_scaling\n",
    "            scaled_adjustment = adjustment * adaptive_scale\n",
    "            \n",
    "            adjusted_logits = gnn_logits + scaled_adjustment\n",
    "            \n",
    "            return adjusted_logits, {\n",
    "                'rule_outputs': rule_outputs,\n",
    "                'adjustment': scaled_adjustment,\n",
    "                'scaling_factor': adaptive_scale\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            return gnn_logits, {'rule_outputs': torch.zeros_like(gnn_logits)}\n",
    "\n",
    "class Optimized58MillionModel(nn.Module):\n",
    "    \"\"\"Optimized 5-8 Million Parameter Model\"\"\"\n",
    "    def __init__(self, sequence_features, problem_features, skill_features, device='cuda'):\n",
    "        super(Optimized58MillionModel, self).__init__()\n",
    "        self.device = device\n",
    "        \n",
    "        self.gnn = OptimizedGNN58M(\n",
    "            sequence_features, problem_features, skill_features,\n",
    "            hidden_dim=320, num_layers=5, num_heads=20, dropout=0.16, device=device\n",
    "        )\n",
    "        self.symbolic_engine = OptimizedSymbolicEngine58M(device)\n",
    "        \n",
    "        # Balanced weights\n",
    "        self.neural_weight = nn.Parameter(torch.tensor(0.32, device=device))\n",
    "        self.symbolic_weight = nn.Parameter(torch.tensor(0.68, device=device))\n",
    "        \n",
    "        # Simplified meta combiner\n",
    "        self.meta_combiner = nn.Sequential(\n",
    "            nn.Linear(3, 96),  # Reduced from 128\n",
    "            nn.BatchNorm1d(96),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.15),\n",
    "            nn.Linear(96, 32),  # Reduced from 64\n",
    "            nn.GELU(),\n",
    "            nn.Linear(32, 1),\n",
    "            nn.Tanh()\n",
    "        ).to(device)\n",
    "        \n",
    "    def forward(self, data):\n",
    "        try:\n",
    "            gnn_logits, gnn_info = self.gnn(data)\n",
    "            symbolic_logits, symbolic_info = self.symbolic_engine(gnn_logits, data)\n",
    "            \n",
    "            weights_sum = torch.abs(self.neural_weight) + torch.abs(self.symbolic_weight) + 1e-8\n",
    "            \n",
    "            neural_contrib = (torch.abs(self.neural_weight) / weights_sum) * gnn_logits\n",
    "            symbolic_contrib = (torch.abs(self.symbolic_weight) / weights_sum) * symbolic_logits\n",
    "            \n",
    "            gnn_probs = torch.sigmoid(gnn_logits)\n",
    "            symbolic_probs = torch.sigmoid(symbolic_logits)\n",
    "            confidence = torch.abs(gnn_probs - 0.5) * 2\n",
    "            \n",
    "            if gnn_probs.shape[0] > 1:\n",
    "                meta_input = torch.stack([gnn_probs, symbolic_probs, confidence], dim=1)\n",
    "                meta_weight = self.meta_combiner(meta_input).squeeze(1)\n",
    "                meta_adjustment = meta_weight * (symbolic_logits - gnn_logits)\n",
    "            else:\n",
    "                meta_adjustment = 0.0\n",
    "            \n",
    "            final_logits = neural_contrib + symbolic_contrib + 0.25 * meta_adjustment\n",
    "            \n",
    "            return final_logits, {\n",
    "                'gnn_logits': gnn_logits,\n",
    "                'symbolic_logits': symbolic_logits,\n",
    "                'symbolic_info': symbolic_info,\n",
    "                'neural_weight': torch.abs(self.neural_weight),\n",
    "                'symbolic_weight': torch.abs(self.symbolic_weight)\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            batch_size = data['sequence'].x.shape[0] if 'sequence' in data else 1\n",
    "            return torch.zeros(batch_size, device=self.device), {}\n",
    "\n",
    "def format_time(seconds):\n",
    "    \"\"\"Format seconds into human readable time\"\"\"\n",
    "    if seconds < 60:\n",
    "        return f\"{seconds:.1f}s\"\n",
    "    elif seconds < 3600:\n",
    "        mins = seconds / 60\n",
    "        return f\"{mins:.1f}m\"\n",
    "    else:\n",
    "        hours = seconds / 3600\n",
    "        return f\"{hours:.1f}h\"\n",
    "\n",
    "def optimized_58m_training_with_progress(model_path, device='cuda'):\n",
    "    \"\"\"OPTIMIZED 5-8M Training with PROGRESS BAR and EVERY EPOCH RESULTS\"\"\"\n",
    "    print(\"🚀💎 OPTIMIZED 5-8M PARAMETER TRAINING WITH PROGRESS BAR 💎🚀\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    try:\n",
    "        # Data loading\n",
    "        print(\"📂 Loading data...\")\n",
    "        start_time = time.time()\n",
    "        saved_data = torch.load(model_path, map_location='cpu')\n",
    "        graph_data = saved_data['graph_data']\n",
    "        targets = saved_data['targets']\n",
    "        \n",
    "        # Device transfer\n",
    "        print(\"🔄 Transferring to GPU...\")\n",
    "        for key in graph_data.keys():\n",
    "            if hasattr(graph_data[key], 'x') and graph_data[key].x is not None:\n",
    "                graph_data[key].x = graph_data[key].x.to(device)\n",
    "            if hasattr(graph_data[key], 'edge_index') and graph_data[key].edge_index is not None:\n",
    "                graph_data[key].edge_index = graph_data[key].edge_index.to(device)\n",
    "            if hasattr(graph_data[key], 'edge_attr') and graph_data[key].edge_attr is not None:\n",
    "                graph_data[key].edge_attr = graph_data[key].edge_attr.to(device)\n",
    "        \n",
    "        if hasattr(graph_data, 'edge_index_dict'):\n",
    "            edge_index_dict = {}\n",
    "            for edge_type, edge_index in graph_data.edge_index_dict.items():\n",
    "                edge_index_dict[edge_type] = edge_index.to(device)\n",
    "            graph_data.edge_index_dict = edge_index_dict\n",
    "        \n",
    "        targets = targets.to(device)\n",
    "        \n",
    "        # Data split\n",
    "        pos_indices = torch.where(targets == 1)[0]\n",
    "        neg_indices = torch.where(targets == 0)[0]\n",
    "        \n",
    "        pos_perm = torch.randperm(len(pos_indices))\n",
    "        neg_perm = torch.randperm(len(neg_indices))\n",
    "        \n",
    "        pos_train_size = int(0.8 * len(pos_indices))\n",
    "        neg_train_size = int(0.8 * len(neg_indices))\n",
    "        \n",
    "        train_indices = torch.cat([\n",
    "            pos_indices[pos_perm[:pos_train_size]],\n",
    "            neg_indices[neg_perm[:neg_train_size]]\n",
    "        ])\n",
    "        val_indices = torch.cat([\n",
    "            pos_indices[pos_perm[pos_train_size:]],\n",
    "            neg_indices[neg_perm[neg_train_size:]]\n",
    "        ])\n",
    "        \n",
    "        train_targets = targets[train_indices]\n",
    "        val_targets = targets[val_indices]\n",
    "        \n",
    "        print(f\"✅ Setup complete: {len(train_targets)} train, {len(val_targets)} val\")\n",
    "        print(f\"   Train balance: {train_targets.float().mean():.3f}\")\n",
    "        print(f\"   Val balance: {val_targets.float().mean():.3f}\")\n",
    "        \n",
    "        # Create optimized model\n",
    "        sequence_features = graph_data['sequence'].x.shape[1]\n",
    "        problem_features = graph_data['problem'].x.shape[1]\n",
    "        skill_features = graph_data['skill'].x.shape[1]\n",
    "        \n",
    "        optimized_model = Optimized58MillionModel(sequence_features, problem_features, skill_features, device).to(device)\n",
    "        \n",
    "        total_params = sum(p.numel() for p in optimized_model.parameters())\n",
    "        trainable_params = sum(p.numel() for p in optimized_model.parameters() if p.requires_grad)\n",
    "        \n",
    "        print(f\"💎 OPTIMIZED Model created:\")\n",
    "        print(f\"   Total parameters: {total_params:,} ({total_params/1e6:.2f}M)\")\n",
    "        print(f\"   Trainable parameters: {trainable_params:,} ({trainable_params/1e6:.2f}M)\")\n",
    "        print(f\"   Target range: 5-8M ✅\" if 5e6 <= total_params <= 8e6 else f\"   ⚠️ Outside 5-8M range\")\n",
    "        \n",
    "        # Optimized optimizer\n",
    "        optimizer = torch.optim.AdamW([\n",
    "            {'params': optimized_model.gnn.parameters(), 'lr': 0.0008, 'weight_decay': 8e-5},\n",
    "            {'params': optimized_model.symbolic_engine.parameters(), 'lr': 0.005, 'weight_decay': 4e-5},\n",
    "            {'params': optimized_model.meta_combiner.parameters(), 'lr': 0.003, 'weight_decay': 6e-5},\n",
    "            {'params': [optimized_model.neural_weight, optimized_model.symbolic_weight], 'lr': 0.005}\n",
    "        ])\n",
    "        \n",
    "        criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(1.15, device=device))\n",
    "        scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            optimizer, max_lr=[0.0016, 0.01, 0.006, 0.01], \n",
    "            epochs=400, steps_per_epoch=1, pct_start=0.05, div_factor=4, final_div_factor=80\n",
    "        )\n",
    "        \n",
    "        # Training setup\n",
    "        best_acc = 0.0\n",
    "        best_epoch = 0\n",
    "        patience = 0\n",
    "        max_patience = 40\n",
    "        training_start_time = time.time()\n",
    "        epoch_times = []\n",
    "        \n",
    "        print(\"=\" * 80)\n",
    "        print(\"🚀 STARTING OPTIMIZED TRAINING WITH DETAILED PROGRESS\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"{'Epoch':<6} {'Time':<8} {'Loss':<8} {'Val Acc':<8} {'Best Acc':<9} {'LR':<10} {'Status'}\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        # Training loop with progress bar\n",
    "        progress_bar = tqdm(range(400), desc=\"🚀 Optimized Training\", unit=\"epoch\")\n",
    "        \n",
    "        for epoch in progress_bar:\n",
    "            epoch_start_time = time.time()\n",
    "            optimized_model.train()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            try:\n",
    "                # Forward pass\n",
    "                all_logits, model_info = optimized_model(graph_data)\n",
    "                train_logits = all_logits[train_indices]\n",
    "                \n",
    "                # Loss with enhanced smoothing\n",
    "                smoothed_targets = train_targets.float() * 0.92 + 0.04\n",
    "                loss = criterion(train_logits, smoothed_targets)\n",
    "                \n",
    "                # Backward\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(optimized_model.parameters(), 1.2)\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                \n",
    "                # Current learning rate\n",
    "                current_lr = optimizer.param_groups[1]['lr']\n",
    "                \n",
    "                # Validation EVERY EPOCH\n",
    "                optimized_model.eval()\n",
    "                with torch.no_grad():\n",
    "                    val_logits, _ = optimized_model(graph_data)\n",
    "                    val_probs = torch.sigmoid(val_logits[val_indices])\n",
    "                    \n",
    "                    # Multi-threshold evaluation\n",
    "                    best_threshold_acc = 0\n",
    "                    best_threshold = 0.5\n",
    "                    for threshold in [0.42, 0.45, 0.48, 0.5, 0.52, 0.55, 0.58]:\n",
    "                        acc = ((val_probs > threshold) == val_targets).float().mean().item()\n",
    "                        if acc > best_threshold_acc:\n",
    "                            best_threshold_acc = acc\n",
    "                            best_threshold = threshold\n",
    "                    \n",
    "                    # Track best model\n",
    "                    status = \"\"\n",
    "                    if best_threshold_acc > best_acc + 0.0008:\n",
    "                        best_acc = best_threshold_acc\n",
    "                        best_epoch = epoch\n",
    "                        patience = 0\n",
    "                        torch.save({\n",
    "                            'model_state_dict': optimized_model.state_dict(),\n",
    "                            'epoch': epoch,\n",
    "                            'best_acc': best_acc,\n",
    "                            'threshold': best_threshold,\n",
    "                            'total_params': total_params\n",
    "                        }, 'optimized_58m_model.pt')\n",
    "                        status = \"🏆 NEW BEST!\"\n",
    "                    else:\n",
    "                        patience += 1\n",
    "                        if patience <= 5:\n",
    "                            status = f\"📈 ({patience})\"\n",
    "                        elif patience <= 15:\n",
    "                            status = f\"⏳ ({patience})\"\n",
    "                        else:\n",
    "                            status = f\"⚠️ ({patience})\"\n",
    "                \n",
    "                # Calculate epoch time\n",
    "                epoch_time = time.time() - epoch_start_time\n",
    "                epoch_times.append(epoch_time)\n",
    "                avg_epoch_time = sum(epoch_times[-10:]) / len(epoch_times[-10:])\n",
    "                \n",
    "                # Estimate remaining time\n",
    "                remaining_epochs = 400 - epoch - 1\n",
    "                estimated_remaining = remaining_epochs * avg_epoch_time\n",
    "                \n",
    "                # Print detailed results\n",
    "                print(f\"{epoch+1:<6} {format_time(epoch_time):<8} {loss.item():<8.4f} \"\n",
    "                      f\"{best_threshold_acc*100:<8.2f}% {best_acc*100:<8.2f}% \"\n",
    "                      f\"{current_lr:<10.2e} {status}\")\n",
    "                \n",
    "                # Update progress bar\n",
    "                progress_bar.set_postfix({\n",
    "                    'Loss': f'{loss.item():.4f}',\n",
    "                    'Val': f'{best_threshold_acc*100:.2f}%',\n",
    "                    'Best': f'{best_acc*100:.2f}%',\n",
    "                    'Patience': f'{patience}/{max_patience}',\n",
    "                    'ETA': format_time(estimated_remaining)\n",
    "                })\n",
    "                \n",
    "                # Success check\n",
    "                if best_acc >= 0.85:\n",
    "                    print(\"\\n\" + \"🎉\" * 40)\n",
    "                    print(\"🏆 85%+ ACCURACY ACHIEVED! 🏆\")\n",
    "                    print(\"🎉\" * 40)\n",
    "                    break\n",
    "                \n",
    "                # Early stopping\n",
    "                if patience >= max_patience:\n",
    "                    print(f\"\\n⏹️ Early stopping triggered at epoch {epoch+1}\")\n",
    "                    break\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"\\n⚠️ Error at epoch {epoch+1}: {e}\")\n",
    "                torch.cuda.empty_cache()\n",
    "                continue\n",
    "            \n",
    "            # Memory cleanup\n",
    "            if (epoch + 1) % 12 == 0:\n",
    "                torch.cuda.empty_cache()\n",
    "                gc.collect()\n",
    "        \n",
    "        progress_bar.close()\n",
    "        \n",
    "        # Training summary\n",
    "        total_training_time = time.time() - training_start_time\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"🏁 OPTIMIZED TRAINING COMPLETE!\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"📊 TRAINING SUMMARY:\")\n",
    "        print(f\"   Total training time: {format_time(total_training_time)}\")\n",
    "        print(f\"   Average time per epoch: {format_time(sum(epoch_times)/len(epoch_times))}\")\n",
    "        print(f\"   Best epoch: {best_epoch + 1}\")\n",
    "        print(f\"   Best accuracy: {best_acc:.4f} ({best_acc*100:.2f}%)\")\n",
    "        print(f\"   Total epochs trained: {len(epoch_times)}\")\n",
    "        \n",
    "        # Load best model\n",
    "        try:\n",
    "            checkpoint = torch.load('optimized_58m_model.pt')\n",
    "            optimized_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            print(f\"✅ Loaded best model from epoch {checkpoint['epoch']+1}\")\n",
    "        except:\n",
    "            print(\"⚠️ Using final model state\")\n",
    "        \n",
    "        improvement = (best_acc - 0.778) * 100\n",
    "        \n",
    "        print(f\"\\n📈 IMPROVEMENT ANALYSIS:\")\n",
    "        print(f\"   Starting baseline: 77.8%\")\n",
    "        print(f\"   OPTIMIZED result: {best_acc*100:.2f}%\")\n",
    "        print(f\"   Net improvement: +{improvement:.2f} percentage points\")\n",
    "        print(f\"   Model parameters: {total_params:,} ({total_params/1e6:.2f}M)\")\n",
    "        print(f\"   Parameter efficiency: {improvement/(total_params/1e6):.2f} pp/M params\")\n",
    "        \n",
    "        if best_acc >= 0.85:\n",
    "            print(\"\\n🎉💎🏆 OPTIMIZED SUCCESS! 85%+ TARGET ACHIEVED! 🏆💎🎉\")\n",
    "        elif best_acc >= 0.82:\n",
    "            print(\"\\n🎊💎 OPTIMIZED EXCELLENCE! Very close to 85%!\")\n",
    "        else:\n",
    "            print(f\"\\n💎 OPTIMIZED achievement: {best_acc*100:.2f}% with {total_params/1e6:.1f}M parameters!\")\n",
    "        \n",
    "        return optimized_model, best_acc\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Training failed: {e}\")\n",
    "        return None, 0.0\n",
    "\n",
    "# ===== LAUNCH OPTIMIZED TRAINING WITH PROGRESS =====\n",
    "print(\"🚀💎 LAUNCHING OPTIMIZED 5-8M PARAMETER TRAINING 💎🚀\")\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "# Run training\n",
    "try:\n",
    "    optimized_model, optimized_accuracy = optimized_58m_training_with_progress(\n",
    "        model_path='./teg_nesynet_models/teg_nesynet_temporal_v1.pt',\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n💎🎯 FINAL OPTIMIZED STATUS:\")\n",
    "    if optimized_accuracy >= 0.85:\n",
    "        print(f\"🏆 OPTIMIZED SUCCESS! {optimized_accuracy*100:.2f}% ACHIEVED!\")\n",
    "    else:\n",
    "        print(f\"💎 Optimized progress: {optimized_accuracy*100:.2f}%\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ Optimization failed: {e}\")\n",
    "    print(\"🔧 Please restart kernel and try again\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9befc73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "print(\"hello\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
